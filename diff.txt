1,3d0
< 
< \RequirePackage[hyphens]{url}
< %\documentclass[10pt,twocolumn]{article}
5d1
< 
8,14c4,5
< \usepackage{pdflscape}
< %\usepackage[tablesonly]{endfloat}
< \usepackage{fancyhdr}
< 
< 
< 
< \usepackage[T1]{fontenc}
---
> \usepackage{dtk-logos}
> \usepackage{amssymb}
22c13
< \usepackage{xcolor}
---
> 
24,26c15
< \usetikzlibrary{arrows}
< \usetikzlibrary{mindmap,trees}
< \usetikzlibrary{backgrounds,shapes,arrows,positioning,calc,snakes,fit}
---
> \usetikzlibrary{arrows,mindmap,trees,backgrounds,shapes,arrows,positioning,calc,snakes,fit}
28,31c17,19
< \usepackage{color, colortbl}
< \usepackage[T1]{fontenc}
< %\usepackage[table]{xcolor}
< \definecolor{Gray}{gray}{0.9}
---
> 
> 
> \usepackage{verbatim}
35,37d22
< \usepackage[figuresright]{rotating}
<  
< \usetikzlibrary{arrows.meta,shadows}
39c24
< \usepackage{makecell}
---
> \usetikzlibrary{arrows.meta,shadows}
48d32
<   
56c40
<  skan tree/.style={
---
>   skan tree/.style={
85,97d68
< \newcolumntype{g}{>{\raggedright\arraybackslash\columncolor{Gray}}p}
< 
< \newcommand{\TITLE}{Cloud Resource Scheduling Taxonomy }
< 
< \author[label1]{Gregor von Laszewski\corref{cor1}\fnref{label3}}
< \address[iu]{{\small $^1$ Intelligent Systems Engineering Dep., Indiana University, Bloomington, IN 47408, USA.}
< }
< \cortext[cor1]{Corresponding author}
< \ead{laszewski@gmail.com}
< \ead[url]{https://laszewski.github.io/}
< 
< \author[label2]{Rajni Aron}
< \address[punjab]{School of Computer Science and Engineering, Lovely Professional University, Punjab, India}
99,115c70
< 
< \author[label1]{Geoffrey C. Fox}
< 
< 
< 
< % in final version remove the following line, it is included
< % so we can review better
< \usepackage[nomarkers,tablesonly]{endfloat}
< 
< %\makeatletter
< %\def\ps@pprintTitle{%
< %   \let\@oddhead\@empty
< %   \let\@evenhead\@empty
< %   \let\@oddfoot\@empty
< %   \let\@evenfoot\@oddfoot
< %}
< %\makeatother
---
> %\journal{FGCS}
118,127c73,77
< \newenvironment{rotatepage}
<         {%
<             \if@twoside%
<                 \ifthispageodd{\pagebreak[4]\global\pdfpageattr\expandafter{\the\pdfpageattr/Rotate 90}}{%
<                 \pagebreak[4]\global\pdfpageattr\expandafter{\the\pdfpageattr/Rotate 270}}%
<             \else%
<                 \pagebreak[4]\global\pdfpageattr\expandafter{\the\pdfpageattr/Rotate 90}%
<             \fi%
<         }%
<         {\pagebreak[4]\global\pdfpageattr\expandafter{\the\pdfpageattr/Rotate 0}}%
---
> \def\ps@pprintTitle{%
>  \let\@oddhead\@empty
>  \let\@evenhead\@empty
>  \def\@oddfoot{}%
>  \let\@evenfoot\@oddfoot}
130,132d79
< \BeforeBeginEnvironment{sidewaystable}{\begin{rotatepage}}
<   \AfterEndEnvironment{sidewaystable}{\end{rotatepage}}
< 
135,148d81
< \onecolumn
< 
< %\parindent0pt Draft:\\
< 
< %{\Large \TITLE\\}
< 
< %\setcounter{tocdepth}{3}
< %\tableofcontents
< %\newpage
< 
< %\twocolumn
< 
< 
< 
150d82
< \title{\TITLE}
152c84
< \maketitle
---
> \title{Cloud Resource Scheduling Taxonomy}
153a86,90
> \author[iu]{Gregor von Laszewski\corref{cor1}}
> \address[iu]{{\small $^1$ Intelligent Systems Engineering Dep., Indiana University, Bloomington, IN 47408, USA.}}
> \ead{laszewski@gmail.com}
> \ead[url]{https://laszewski.github.io/}
> \cortext[cor1]{Corresponding author}
154a92,93
> \author[punjab]{Rajni Aron}
> \address[punjab]{School of Computer Science and Engineering, Lovely Professional University, Punjab, India}
156d94
< \begin{keyword}
158,162c96
<  Y-Cloud Taxonomy,
<  Cloud Scheduling,
<  Scheduling Virtual Machines,
<  Scheduling Containers,
<  Scheduling FaaS
---
> \author[iu]{Geoffrey C. Fox}
164d97
< \end{keyword}
167,169c100,114
< 
< The growth and development of commercial and scientific applications in the cloud demand the creation of efficient resource management systems to coordinate the resources while addressing the heterogeneity of services, the inter-dependencies, and unpredictability of load posed by the users.
< We present a resource scheduling taxonomy that originates from the experience of the authors in utilizing and managing multi-cloud environments. This study is backed up by a literature review that targets not only virtual machines but also container and Function as a Service frameworks. It justifies a proposed resource provider focused Y-cloud taxonomy and introduces an overview of existing scheduling techniques in cloud computing. As a result, this work can lead to a better understanding of the complex field of scheduling for clouds in general. Furthermore, the study promotes through the Y-cloud taxonomy, the vision of a layered scheduling architecture that will be useful for the implementation of application and resource-based scheduling frameworks in support of the NIST Big Data Reference Architecture.
---
> The growth and development of scientific applications in the cloud
> demands the creation of efficient resource management systems. Due to
> the scale of resources, the heterogeneity of services, their
> inter-dependencies and unpredictability of load this is a complex
> problem. We present a resource scheduling taxonomy that is originating
> from our experience in utilizing and managing multi-cloud environments. 
> Our study is backed up by a literature review that
> targets not only virtual machine, but also container and Function as a
> Service frameworks. It justifies our model and provides a an overview
> of existing scheduling techniques in cloud computing. As a result this
> work can lead a better understanding of scheduling for clouds
> in general. The study promotes the vision of a layered scheduling
> architecture that will be useful for the implementation of application
> and resource-based scheduling frameworks in support of the NIST Big
> Data Reference Architecture.
172a118,129
> \begin{keyword}
>   Taxonomy \sep
>   Scheduling \sep
>   Cloud \sep
>   Containers \sep
>   HPC in the Cloud \sep
>   Virtual Machines \sep
>   FaaS \sep
>   IaaS \sep
>   PaaS
> \end{keyword}
> 
179,198d135
< Cloud computing has emerged as a computing paradigm to fulfill large-scale application requirements in domains including science, e-commerce, lifestyle, and many other fields. According to the definition of NIST, {\em Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction}~\cite{mell2011nist}.
< 
< Sustaining efficient resource provisioning and utilization in clouds is a formidable challenge. Poor resource management results in high costs that are amplified by long term and dynamic resource usage we see in many cloud applications. Hence, scheduling plays an important role in improving resource utilization and optimization. Consequently, resource scheduling is an important service of any cloud framework as it is responsible for orchestrating the resources to both cloud providers and cloud users in an efficient manner.
< 
< In this paper, we contribute to the argument that scheduling in the cloud requires a multi-layered approach that not only schedules tasks and jobs but also integrates resource provisioning and dynamic resource adaptation during the runtime of cloud applications. Information has to be passed between the various layers that comprise this scheduling architecture for clouds to guide the optimal placement onto resources. Hence, a cloud-based scheduling model is more comprehensive than previous classical scheduling approaches as it is conducted on scales and types of resources that were previously not considered. Scheduling is not only done on the task, job, and cluster-level but integrates the data center and even regional and global data centers while adding on-demand resource needs. To work towards a layered scheduling model we have introduced a Y-Cloud-Taxonomy that allows us to work towards the identification and implementation of scheduling models and algorithms at different junction points. Furthermore, this study already contributed considerably to the identification of services that assist in the formulation of the scheduling needs and interfaces with the NIST Big Data Reference Architecture (NIST-BDRA)~\cite{nist-bdra-vol6} definitions currently under development~\cite{nist-bdra-vol8}.
< 
< The paper is structured as follows. In Section~\ref{sec:terminology} the terminology used in the paper is introduced. Next, we present in Section ~\ref{sec:taxonomy} an architecture view and taxonomy that we derived from the practical experience with FutureGrid \cite{las12fg-bookchapter}, FutureSystem, and Software such as Cloudmesh~\cite{von2014accessing}, Virtual Clusters \cite{las-comet}, and Rain~\cite{las-fg-1295,las10dynamic,las-rain} while working on hybrid and multi-cloud frameworks.
< 
< This view is backed up by an extensive literature review presented in Sections \ref{sec:literature} and their classification based on the taxonomy introduced in Section~\ref{sec:taxonomy}. Lastly, we provide some concluding remarks in Section~\ref{sec:conclusion}.
< 
< 
< \subsection{Contributions}
< 
< The contributions of this paper are the following:
< 
< \begin{itemize}
<  
< \item We introduce a resource provider focussed Y-Cloud Taxonomy \ref{sec:y} that establishes a provider view associating physical, resource, and connectivity models for clouds with each other (Section \ref{sec:y}). This view helps to implement a layered scheduling approach.
< 
< \item We identify specific characteristics we face in cloud computing that provide specific scheduling challenges motivated by the use of clouds.
200c137,184
< \item We introduce a detailed general classification of cloud scheduling while analyzing clouds in regards to the cloud infrastructure, the models to describe and utilize the cloud infrastructure efficiently, and categorize scheduling frameworks and algorithms to address the many scheduling problems arising in the cloud.
---
> Cloud computing has emerged as a computing paradigm to solve
> large-scale application in many domains including science, e-commerce, 
> lifestyle, and many other areas. According to the definition of NIST, 
> Cloud
> computing is a model for enabling ubiquitous, convenient, on-demand
> network access to a shared pool of configurable computing resources
> that can be rapidly provisioned and released with minimal management
> effort or service provider interaction~\cite{mell2011nist}.
> 
> Sustaining efficient resource provisioning and utilization in clouds
> is a formidable challenge. Poor resource management results in high
> costs that are amplified by long term and dynamic-scalable usage
> patterns we see in large scale cloud applications. Hence, scheduling
> plays an important role in improving resource utilization providing
> the necessary guidance to optimize the allocation of resource.
> Consequently, Resource scheduling is an important service of any cloud
> framework as it is responsible for orchestrating the resources to both
> cloud providers and cloud users in an efficient manner.
> 
> We showcase that scheduling in the cloud requires a multi-layered
> approach that not only schedules tasks and jobs, but also integrates
> provisioning of resources and dynamic adaptation of loads during the
> run-time of applications. Information has to be passed between the
> various layers that comprise our scheduling architecture for clouds to
> guide the optimal placement onto resources. This cloud-based
> scheduling models is more comprehensive than previous classical
> scheduling approaches as it is conducted on scales and resource pools
> that were previously not considered. The scheduling is not only done
> on the task, job, and cluster level but integrates the data center and
> even regional and global data centers while adding on demand resource
> needs. Our study identifies services that assist in the formulation of
> the scheduling needs and interfaces for the NIST Big Data Reference
> Architecture (NIST-BDRA) \cite{nist-bdra-vol6} and it's interface
> definitions currently under development \cite{nist-bdra-vol8}.
> 
> The paper is structured as follows. We present first in
> Section~\ref{sec:terminology} our general terminology we use throughout
> the paper. Next we present in Section ~\ref{sec:taxonomy} our
> architecture view and taxonomy that we have derived from our practical
> experience with FutureGrid
> \cite{las12fg-bookchapter,fox2013futuregrid}, FutureSystem, and
> Software such as Cloudmesh \cite{von2014accessing}, Virtual Clusters
> \cite{las-comet}, and Rain \cite{las-fg-1295,las10dynamic,las-rain}
> while working on multi-hosted heterogeneous cloud frameworks. This
> view is backed up by our extensive literature research in Sections
> \ref{sec:literature} and our classification based on the taxonomy we
> introduced in the previous sections. Lastly, we provide some
> concluding remarks in Section~\ref{sec:conclusion}.
202d185
< \item Based on the lessons learned while being a resource provider for clouds, a developer and a researcher of cloud software and applications we identified that a layered and phased scheduling model is beneficial. The benefits of such a model include the separation of scheduling concerns between infrastructure, platform, software, and function as a Service while at the same time projecting a holistic approach.
204,215c187
< \item We provide a systematic survey of cloud scheduling approaches and associate them with the presented scheduling taxonomy.
< 
< \item We identify areas that have not yet been addressed by this paper and outline future activities.
< \end{itemize}
< 
< 
< 
< \section{Terminology and Basic Concepts}\label{sec:terminology}
< 
< In this section, terminology and basic concepts related to cloud and scheduling are discussed.
< 
< \subsection{General Scheduling Terminology for Clouds}
---
> \section{General Scheduling Terminology for Clouds}\label{sec:terminology}
221,282c193,217
< \item[Cloud Computing] is according to the definition of NIST, Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction~\cite{mell2011nist}.
< 
< \item[Cloud Resource] is a resource offered by a cloud provider on which cloud services are run as part of the implementation of a cloud application that may use this resource.
< 
< \item[Cloud Service] is a service offered by a cloud provider or developed as part of an application utilizing cloud resources and exposing the functionality as a service.
<  
< \item[Cloud Application] is an application that uses cloud services and resources for its instantiation and execution.
< 
< \item[Resource Provisioning in the Cloud] is the process of allocating resources demanded by services and applications running in the cloud.
<  
< \item[Resource Scheduling in the Cloud] refers to the mapping of resources to fulfill the cloud service requirements.
< 
< \item[Cloud Scheduler] refers to a service that maps basic cloud scheduling units such as virtual machines, containers, functions, and data onto cloud resources to utilize them while leveraging a scheduling policy. 
< 
< \item[Cloud Scheduling Policy] refers to a policy employed by the scheduler to derive decisions as to how to guide a scheduling algorithm.
< 
< \item[Cloud Scheduling Algorithm] refers to an algorithm that includes cloud scheduling units and policies (as defined next), and resources as input and determines an optimized mapping of cloud scheduling units to cloud resources.
<  
< \end{description}
< 
< \subsection{Scheduling Units}
< 
< The traditional units for scheduling include processes, tasks, and jobs. However, in the cloud, it is beneficial to consider an enhanced set of scheduling units. These units must include scheduling of virtual machines, containers, functions, platforms, clusters, services, and other infrastructure or services used by the clients or cloud-related services. Naturally, such units can be abstracted into tasks that are coordinated as part of cloud workflows.
< 
< Hence, we distinguish the following scheduling units related to cloud computing:
< 
< \begin{description}
< 
< \item[Task] is an abstract unit to be run on a cloud that may have complex resource requirements attached to them and may itself be built from other tasks. 
< 
< \item[Job] is a computational activity made up of several tasks that may require different processing capabilities while resolving the resource requirements as part of a scheduling process.
< 
< \item[Function] is a small computational unit executed as service with precisely specified resource requirements to run on a cloud. Please note that to distinguish them from the common term we also refer to them as Function as a Service.
< 
< \item[Application] is a software solution for solving a (large) problem in a computational infrastructure. Applications may require splitting the use of any combination of tasks, jobs, services, and functions while using Cloud resources to solve the requirements of the applications. The allocation of resources is usually referred to as application deployment.
< 
< \item[Workflow] contains a combination of Tasks, Jobs, Functions, and applications with dependencies assuring the order of execution.
< 
< \end{description}
< 
< Tasks, services, functions, and applications must be mapped
< onto cloud resources to be able to be executed. The association of
< such resources is typically conducted in the resource provisioning. We
< list next the terminology related to provisioning:
< 
< \begin{description}
< 
< \item[Resource] is a basic computational entity that can be used to fulfill the requirements of the application's execution. Resources have specific characteristics such as CPU, memory, software, disks, etc. Various performance and policy parameters are associated with a resource, among them, the data speed, the processing speed, space, and workload, which change over time, as well as cost, authentication, and authorization policies.
< 
< \item[Deployment] is a series of jobs that deploy services onto the cloud that can be used for subsequent use as part of an application or service.
< 
< \item[Container] is an agglomeration of software that includes all packages and dependencies so it can be run easily on cloud computing resources due to its standardized specification.
< 
< \item[Virtual machine (VM)] is a simple software program that simulates the functions of a physical machine.
< 
< \item[Virtual cluster] is an agglomeration of virtual services that build the core of a computational resource hosted in the cloud. A virtual cluster can be comprised out of many resources including virtual machines, containers, Platform as a Service frameworks, data services, and resources, and more. A virtual cluster may be associated with an application and optimized for its use. Just as containers or virtual machines, a virtual cluster can be created, suspended, resumed, or terminated. 
< 
< \item[Scheduler] is a process that decides which task and process should be accessed and run at a specific time by the resources. Schedulers help to keep the performance of the cloud at the highest level by using optimization strategies. Based on the scope of resources involved in the scheduling decision we distinguish between global, regional, and local schedulers.
<  
< \item[Task, Job, Application, Service, Function scheduling] is to allocate resources to a particular scheduling unit so they can be executed. Limited resource availability and their cost motivate the development of optimized scheduling algorithms to address the problem of task scheduling.
<  
< \item[Provisioning] is a process to aggregate resources and services that are used as part of the application or software service-related infrastructure setup. Provisioning helps users to simplify the resource management tasks while accessing resources that are hosted in the cloud and made available to the user through provisioning. 
---
> \item[Cloud Computing:] According to the definition of NIST, Cloud computing
>   is a model for enabling ubiquitous, convenient, on-demand network
>   access to a shared pool of configurable computing resources that can
>   be rapidly provisioned and released with minimal management effort
>   or service provider interaction~\cite{mell2011nist}.
> 
> \item[Cloud Resource:] Is a resource offered by a cloud provider as
>   part of the implementation of a cloud application that may use this
>   resource.
> 
> \item[Cloud Service:] Is a service offered by a cloud provider or
>   developed as part of an application utilizing cloud resources and
>   exposing the functionality as a service.
>   
> \item[Cloud Application:] Is an application that uses cloud services
>   and resources to target an application providing concrete
>   implementations for its instantiation and execution.
> 
> \item [Resource Provisioning in the Cloud:] The allocation of the
>   resources demanded by users to specific
>   applications is called provisioning. 
>   
> \item [Resource Scheduling in the Cloud:] Resource scheduling in the
>   cloud refers to the mapping of resources to fulfill the jobs
>   resource requirements.
286,287d220
< 
< 
290,296d222
< In this section, we introduce a scheduling taxonomy for clouds. The taxonomy integrates the classical service-oriented cloud architectures defined by NIST~\cite{mell2011nist}. 
< 
< First, we will introduce a motivation for introducing the concept of layered scheduling that motivates the use of the taxonomy in the separate layers.
< 
< Second, we will be introducing a resource provider focused Y-Cloud taxonomy that deals with showcasing the relationship between cloud resources, their physical instantiation and their connectivity in a layered fashion depicted as a Y-diagram.
< 
< Next, we present in the taxonomy classifications.
297a224,229
> In this section we introduce our scheduling taxonomy for clouds. Our
> taxonomy integrates the classical cloud architecture as defined by
> NIST \cite{mell2011nist}. However, as scheduling is conducted with
> resources in mind we also focus on aspects to deal with cloud
> resources, their physical instantiation and their connectivity while
> showcasing their relationship in our taxonomy.
301c233,239
< The NIST cloud model promotes an easy to understand separation between infrastructure, platforms, and software as a service. This separation motivates a scheduling taxonomy separated by the different layers in which service providers and users attempt to place compute, data and other services in order to optimize the use of the infrastructure as is showcased in Figure~\ref{F:graph-layer}, in which we added also Function as a Service (FaaS) as it is going to be playing a major role in upcoming cloud Software as a Service offerings, just as platforms did.
---
> The NIST cloud model promotes an easy to understand separation between
> infrastructure, platforms and services. This separation motivates a
> scheduling taxonomy separated by the different layers in which service
> providers and users attempt to place compute, data and other services
> in order to optimize the use of the infrastructure as is showcased in
> Figure~\ref{F:NIST} while enhancing it with Function as a Service
> (FaaS).
308,314c246,277
< A platform provider may utilize insights of the infrastructure to offer to the users an optimized platform placement, while a software provider or application user may utilize information from the platform and or the infrastructure to offer to schedule on levels accessible to them. To facilitate the scheduling on the lower levels, scheduling information has to be passed along to them to provide enough information to the provider to integrate scheduling of resources that are not under direct control by the developer and users.
< 
< Thus one strategy to develop scheduling algorithms for the cloud is to integrate the service boundaries of the layered cloud architecture into conducting a multi-layered scheduling approach. In this approach, we separate scheduling concerns related to resources, platform, function and application scheduling as showcased in Figure~\ref{F:graph-flow}. As most recently the FaaS model has gained traction we added it to Figure~\ref{F:graph-flow} to indicate that through the use of resource bound functions scheduling decisions propagated to the infrastructure provider level become easier. Hence to optimize usage of the infrastructure, cloud providers have integrated the use of functions in their portfolio in addition to the original NIST model. This integration allows for the better potential of utilizing the infrastructure by scheduling small well-defined functions with limited resource needs.
< 
< Certainly, the goal of hiding the scheduling decisions between each layer is still important to reduce complexity exposed to the users and developers, but if enough information between the layers is exchanged, this information can lead to good scheduling decisions on each of the layers.
< 
< When putting together, we distinguish several aspects that relate to cloud scheduling. This includes metrics, cloud scheduling models, the cloud infrastructure, and algorithms specifically designed to address clouds as seen in Figure~\ref{F:graph-mindmap}. These aspects are elaborated in more detail next.
---
> A platform provider may utilize insights of the infrastructure to
> offer to the users an optimized platform placement, while a software
> provider or application user may utilize information form the platform
> and or the infrastructure to offer scheduling on levels accessible to
> them. To facilitate the scheduling on the lower levels, scheduling
> information has to be passed along to them to provide enough
> information to the provider to integrate scheduling of resources that
> are not under direct control by the developer and users.
> 
> Thus one strategy to develop scheduling algorithms for the cloud is to
> integrate the service boundaries of the layered cloud architecture
> into the strategy of conducting a multi-layered scheduling approach in
> which we separate concerns as showcased in Figure~\ref{F:multiphase}.
> As most recently the FaaS model is introduced by the community we
> added it to the figure to indicate that although they are defined by
> the user, the functions have well defined resource constraints that
> make scheduling decisions on the infrastructure provider level easier.
> Hence to optimize usage of the infrastructure, providers have come up
> with an easy to use extension to the original NIST model allowing for
> better potential of utilizing the infrastructure by scheduling small
> well defined functions with limited resource needs.
> 
> Certainly the goal of hiding the scheduling decisions between each
> layer is still important, but enough information between the layers
> needs to be exchanged to facilitate good scheduling decisions on each
> of the layers.
> 
> When put together, we distinguish several aspects that comprise Cloud
> scheduling. This includes metrics, cloud scheduling models, cloud
> scheduling challenges, the cloud infrastructure, and algorithms
> specifically designed to address clouds as seen in Figure~{F:mindmap}.
> These aspects are elaborated in more detail next.
318,320c281
< \subsection{Resource Provider Focused Y-Cloud Taxonomy}\label{sec:y}
< 
< To showcase the interaction between the different layers more clearly we like to refer the reader to the Y-cloud scheduling diagram introduced by Laszewski in~\cite{lasbook}.
---
> \subsection{Resource Provider Focused Y-Cloud Taxonomy}
322c283,290
< In this taxonomy, we are concerned about how resources are placed on physical models and are interconnected with each other to facilitate scheduling algorithms. Figure~\ref{F:graph-y} depicts the different models that are an integral part of this taxonomy. It includes the
---
> To showcase the interaction between the different layers more clearly
> we like to refer the reader to the Y-cloud scheduling diagram
> introduced by Laszewski in~\cite{las18cloudscheduling-whitepaper}.
> 
> In this taxonomy we are concerned about how resources are placed on
> physical models and are interconnected with each other to facilitate
> scheduling algorithms. Figure~\ref{F:taxonomy} depicts the different
> models integrated in the Taxonomy. It includes:
326,330c294,310
< \item[Physical Model] representing major physical resource layers to enable a hierarchical scheduling strategy across multiple data centers, racks, servers, and computing cores.
< 
< \item[Resource Model] representing resource-based scheduling decisions while dealing with containers and functions, virtual machines and jobs, virtual clusters, provider-managed resources, and multi-region provider-managed resources.
< 
< \item[Connectivity Model] introducing connectivity between components when addressing scheduling. This includes components such as memory, processes, connectivity to distributed resources, hyper-graphs to formulate hierarchies of provider-based resources, and region enhanced hyper-graphs. The connectivity model allows us to leverage classical scheduling algorithms while applying such models and leveraging established or new scheduling algorithms for these models.
---
> \item[Physical Model] representing major physical resource layers to
>   enable a hierarchical scheduling strategy across multiple data
>   centers, data centers, racks, servers, and computing cores.
> 
> \item[Resource Model] representing models that the scheduling
>   algorithm addresses including containers and functions, virtual
>   machines and jobs, virtual clusters, provider managed resources, and
>   multi-region provider managed resources.
> 
> \item[Connectivity Model] introduces a connectivity between components
>   when addressing scheduling. This includes components such as memory,
>   processes, connectivity to distributed resources, hyper-graphs to
>   formulate hierarchies of provider based resources, and region
>   enhanced hyper-graphs. The connectivity model allows us to leverage
>   classical scheduling algorithms while applying such models and
>   leveraging established or new scheduling algorithms for these
>   models.
338,342c318,339
< Now that we have identified the resource provider focused Y-Cloud taxonomy we can identify some important classifications that govern the scheduling decisions to effectively use these resources. This includes metrics that influence the scheduling. Traditional scheduling metrics and attributes for scheduling algorithms are shown in Figure~\ref{F:graph-metrics}. They include typically cost, time, space, reliability, energy, and security.
< 
< \input{graph-metrics}
< 
< When looking into the specifics of these metrics applied to cloud computing we can easily identify more details for these traditional metrics that apply to the various infrastructure components that constitute a cloud including compute, data, energy, quality of service, and security. We depict some of the major attributes that influence the scheduling decisions in Figure~\ref{F:graph-taxonomy}. Furthermore, each of the attributes in the categories Compute, Data, Security, Energy, and Quality of service can be combined if not already included in the specific scheduling attribute. For example, to identify a scheduling model based on virtual machines, attributes such as those in data, energy, QoS, or security may be introduced in the scheduling decision.
---
> Now that we have identified the resource provider focused Y-Cloud
> taxonomy we can identify some important aspects that govern the
> scheduling decisions to effectively use these resources. This includes
> metrics that influence the scheduling. Traditional scheduling metrics
> and attributes for scheduling algorithms are shown in
> Figure~\ref{F:class-metric}. They include typically cost, time, space,
> reliability, energy, and security.
> 
> % \input{graph-metrics}
> 
> When looking into the specifics of these metrics applied to cloud
> computing we can easily identify more details for these traditional
> metrics that apply to the various infrastructure components that
> constitute a cloud including compute, data, energy, quality of
> service, and security. We depict some of the major attributes that
> influence the scheduling decisions in Figure~\ref{F:class-taxonomy}.
> Furthermore each of the attributes in the categories Compute, Data,
> Security, Energy, and Quality of service can be combined if not
> already included in the specific scheduling attribute. For example in
> order to identify scheduling model based on virtual machines,
> attributes such as the once in data, energy, QoS, or security may be
> introduced in the scheduling decision.
346,348c343,344
< This information can now be used to define provisioning and service scheduling as categorized next.
< 
< \subsection{Challenges in Cloud Scheduling}\label{sec:challange}
---
> This information can now be used to define provisioning and service
> scheduling as categorized next.
350c346
< It is important to understand that cloud scheduling is going beyond traditional scheduling approaches. For this reason, we need to look at specific challenges we face that will lead us to features that need to be addressed by scheduling solutions for the cloud and build significant requirements to be addressed by scheduling solutions
---
> \subsection{Taxonomy of Challenges in Cloud Scheduling}\label{sec:challange}
352c348,351
< Some of the obvious cloud characteristics and challenges are listed next and are summarized in Figure~\ref{F:graph-challenges}.
---
> Some of the obvious characteristics and challenges that are
> specifically related to clouds are listed next and are summarized in
> Figure~\ref{F:metrics-char} while enhancing the traditional scheduling
> challenges we introduced earlier.
356c355,411
< \item [Large scale:] Clouds offer a large number of resources to its users that need to be optimally utilized under the quality of service constraints set by providers and users. A cloud involving a plethora of resources spanning across the globe is obviously a huge infrastructure. The range of functions, tasks, jobs, and applications need to be scheduled at any point of time onto available resources. Handling them on such scale requires efficient resource management. As such, scheduling becomes a complex endeavor, integrating dynamic and multi-faceted scheduling. 
---
> \item [Large scale:] Clouds offer large number of resources to its
>   users that need to be optimally utilized under quality of service
>   constraints set by providers and users. A cloud involving a plethora
>   of resources spanning across the globe is obviously a huge
>   infrastructure. The range of functions, tasks, jobs and applications
>   need to get catered at any point of time too can be in large scale.
>   Handling them in such scale requires efficient resource management.
>   As such, scheduling becomes a complex endeavour. Rather a complex,
>   dynamic and multi-faceted scheduling is necessary.
>             
> \item [Dynamic nature of clouds:] Due to the dynamic nature of the
>   physical cloud infrastructure in which resources belonging to
>   different administrative domains keep on joining and leaving the
>   system scheduling must be adaptive.
> 
> \item[Heterogeneous providers and services:] There is no single cloud,
>   but we have to recognize that the competitive nature in the cloud
>   market promotes not only heterogeneous cloud providers, but also
>   heterogeneous cloud services that may compete with each other and
>   either offer the same or customized services targeting a particular
>   user community. Resources in cloud environment are highly
>   diversified in nature, capacity, working style and administrative
>   domains. Being owned by different organizations, the resources offer
>   minimal control over them making multi-layered scheduling necessary,
> 
> \item [Highly diversified:] Due to the large diverse set of
>   applications (but also infrastructure) smart strategies to schedule
>   such applications on the required resources are needed.
> 
> \item [Decentralized:] The resources in the cloud are distributed
>   among various data centers, rack, and servers. Although they may
>   belong to a provider, they can still be utilized across provider
>   boundaries and even if within the same provider regions, calling for
>   a high degree of decentralization.
> 
> \item[Limited control by users:] Due to the fundamental nature of the
>   cloud access to low level scheduling mechanisms are often hidden and
>   only available to the provider. On the other hand users still have
>   their own scheduling requirements in regards to for example cost,
>   and deadlines.
>   
> \item[Dynamic loads:] Due to the size of the user community sporadic
>   burst on resource requirements lead to challenges to adjust
>   provisioned resources and schedule application onto them.
> 
> \item[Security concerns:] Another important requirement for scheduling
>   is the ability to integrate issues such as privacy and security
>   considerations as the provider needs to assure that local laws as
>   well as the general privacy and security concerns are addressed.
>   This is especially of concern when government or health care
>   providers need to schedule resources in a cloud for their
>   application needs, making it necessary to distinguish problems that
>   can be executed on public vs private clouds through scheduling but
>   also through policy decisions that integrate with scheduling
>   algorithms.
>   
> \end{description}
358c413,434
< \item [Dynamic nature of clouds:] Clouds encompass a dynamically changing resource environment in which resources belong to different administrative domains keep on joining and leaving the clouds. Hence, scheduling must be adaptive and address the dynamic resource availability.
---
> Thus we need to distinguish a number of scheduling challenges one of
> which is governed by on differentiating users and providers. Here, on
> the one hand, we focus on cloud providers that try to utilize in the
> best possible way the existing resources for the customers under
> optimization constraints such as cost, high availability, fault
> tolerance for the providing cloud resources and services. On the other
> hand, we have customers that expect these quality assurances, but also
> have own constraints such as deadlines, cost, and implicit
> requirements from their applications such as data placement and
> management.
> 
> In both cases we need to address the challenge of provisioning
> resources and also the challenge of scheduling services onto these
> resources. Although these steps can be done independently it is
> obvious that interrelationship between them is needed in case of
> re-provisioning and dynamic adaptation to dynamic loads placed on the
> resources.
> 
> In both cases under-utilization prevents a resource from performing
> optimally, incurring idle time, whereas over-utilization causes a
> resource to function more, thereby, sometimes, degrading the node's
> performance.
360c436
< \item[Heterogeneous providers and services:] There is no single cloud. We have to recognize that the competitive nature in the cloud market promotes not only heterogeneous cloud providers but heterogeneous cloud services that may compete with each other and either offer the same or customized services targeting a particular user community. Resources in clouds are highly diversified in nature, capacity, working style, and administrative domains. The inclusion of different resource providers with the desire to lock customers into their services and products makes heterogeneous multi-cloud scheduling a formidable challenge.
---
> \input{graph-challenges}
362d437
< \item [Highly diversified:] Due to the large diverse set of applications (but also infrastructure) smart strategies to schedule such applications on the required resources are needed.
364c439
< \item [Decentralized:] The resources in the cloud are distributed among various data centers, rack, and servers. Although they may belong to a provider, they can still be utilized across provider boundaries and even within the same provider regions, calling for a high degree of decentralization.
---
> \subsection{Scheduling Challenges Arising form use of Containers}
366,368c441,499
< \item[Limited control by users:] Due to the fundamental nature of the cloud, access to low-level scheduling mechanisms is often hidden and only available to the provider. On the other hand, users still have their own scheduling requirements in regards to, for example, cost and deadlines.
<  
< \item[Dynamic loads:] Due to the size of the user community sporadic burst on resource requirements lead to challenges to adjust provisioned resources and schedule application onto them.
---
> By using virtualization technologies such as virtual machines, they
> help to provide the illusion of a hardware resources but introduce a
> cost to also virtualize the operating system. Containers however use
> virtualization within the operating system level. Multiple containers
> run on the top of the operating system kernel. Hence, a container is a
> lightweight approach to implement the virtualization technology
> leveraging the underlying OS. The memory consumption by containers is
> less then the resources required to boot a virtual machine with its
> virtualized OS. As example we point out Kubernetes \cite{Kuber2018}
> where containers within a pod \cite{Kubernates2018} share an IP
> address and find each other via local host. Communication among them
> is done by inter-process communications, such as, SystemV semaphores
> or POSIX shared memory. Containers in different pods cannot
> communicate directly as they have distinct IP addresses. Kubernetes
> commonly uses flannel to accomplish container networking. Containers
> are joined in a virtual network. Kubernetes, provides mechanisms to
> utilize a number of pre-existing scheduling algorithms, but also
> provides the ability to replace them with customized approaches.
> 
> The challenge here is to assure that containers between users do not
> create security or violate privacy issues. Also the access to
> potentially elevated system privileges may cause other issues.
> Therefore systems such as Singularity offer users an isolated use of
> containers within traditional HPC queuing systems to mitigate that
> issue. Still once on such a system, we still have to be aware of
> elevated privileges, and containers may only be offered in limited
> form to its users. Once this has been clarified, also for containers
> the typical quality assertions during its use apply just as for
> virtual machines. Such challenges must be integrated into a scheduling
> strategy when adding containerized cloud resources.
> 
> 
> \subsection{Challenges in Function as a Service}
> 
> The {\em Function as a Service} model allows the developers to build
> and execute their programs through a combination of functions, that
> limit resource requirements. Functions are uploaded to FaaS supporting
> infrastructure and services and triggered by events. Due to the
> resource limitations they provide significant information for the
> underlaying layers to provide more efficient resource scheduling.
> However, monitoring tools and fault tolerance have to be carefully
> integrated in order to avoid FaaS failures based on resource
> starvation or an excess of resources used. In addition more resource
> intense functions may require splitting them up in smaller functions
> so they can be fulfill resource constraints of the FaaS framework.
> Such limitations must be understood by the developer in order not to
> create a function that is impossible to schedule.
> 
> 
> \subsection{Taxonomy of Scheduling Units}
> 
> The traditional units for scheduling include, processes, tasks, and
> jobs. However in the cloud we have an enhanced model that needs in
> addition to this address scheduling of virtual machines, containers,
> functions, platforms, clusters, and other infrastructure or services
> used by the clients or cloud related services. Naturally such units
> can be abstracted into tasks that are coordinated as part of
> workflows. Hence, we distinguish as part of this model the following
> units that typically define work units:
370,372c501
< \item[Security concerns:] Another important requirement for scheduling is the ability to integrate issues such as privacy and security considerations as the provider needs to assure that local laws, as well as, the general privacy and security concerns are addressed. This is especially of concern when government or health providers need to schedule resources in a cloud for their application needs, making it necessary to distinguish problems that can be executed on public vs private clouds through scheduling but also through policy decisions that integrate with scheduling algorithms.
<  
< \end{description}
---
> \begin{description}
374c503,523
< Thus, we need to distinguish many scheduling challenges, one of which is governed by differentiating users and providers. Here, on the one hand, we focus on cloud providers that try to utilize in the best possible way to utilize the existing resources for the customers under optimization constraints such as cost, high availability, fault tolerance for the providing cloud resources and services. On the other hand, we have customers that expect quality assurances, but also have their own constraints such as deadlines, cost, and implicit requirements from their applications including data placement and management that may influence the scheduling decision.
---
> \item[Task:] represents an abstract unit to be run on a cloud that may
>   have complex resource association attached with it and may itself be
>   build from other tasks with dependencies. It is not yet mapped onto
>   a resource.
> 
> \item[Job:] A job is a computational activity made up of several tasks
>   that may require different processing capabilities.
> 
> \item[Function:] represents a small computational unit with precisely
>   specified resource requirements to run on a cloud.
> 
> \item[Application:] An application is a software for solving a (large)
>   problem in a computational infrastructure; it may require splitting
>   the computation into many jobs or it may be a monolithic
>   application. In the later case, the whole application is allocated
>   in a computational node and is usually referred to as application
>   deployment.
> 
> \item[Workflow:] A workflow contains a combination of Tasks, Jobs,
> Functions, and applications with dependencies assuring the order of
> execution.
376c525
< In both cases, we need to address the challenge of provisioning resources and also the challenge of scheduling services onto these resources. Although they can be done independently, it is obvious that interrelationship between them is needed in case of re-provisioning and dynamic adaptation to dynamic loads placed on the resources.
---
> \end{description}
378c527,528
< In both cases under-utilization prevents a resource from performing optimally, incurring idle time, whereas over-utilization causes a resource to degrade the node's performance.
---
> On the other hand we find resources units that are typically
> associated with the provisioning step:
380c530
< \input{graph-challenges}
---
> \begin{description}
381a532,564
> \item[Resource:] A resource is a basic computational entity that can
>   be used to fulfill the requirements of application's execution.
>   Resources have their own characteristics such as CPU
>   characteristics, memory, software, etc. It can be a source of
>   information and expertise. Resource is phenomenon that enhances the
>   quality of application. Various parameters are associated with a
>   resource, among them, the data speed, the processing speed, space
>   and workload, which change over time.
> 
> \item[Deployment:] A deployment is a series of jobs that deploy a
>   service onto the cloud that can be used for subsequent use.
> 
> \item[Containers:] can be defined as a any service related to
>   scheduling, made available to users on demand from a cloud computing
>   provider's servers.
> 
> \item[Virtual machine:] Virtual Machine (VM) is a simple software
>   program which simulates the functions of a physical machine.
> 
> \item[Virtual clusters:] An agglomeration of virtual services that
>   build the core of a computational resource hosted in the cloud. They
>   can be comprised out of many resources including virtual machines,
>   containers, platform as a service frameworks, data services and
>   resources, and more. A virtual cluster is typically associated with
>   an application and utilized for it. Just as containers or virtual
>   machines it can be created, suspended, resumed, or terminated
> 
> \item[Schedulers:] Schedulers are processes that decide which task and
>   process should be accessed and run at what time by the available
>   resources. It helps to keep the performance of cloud at the highest
>   level by scheduling in optimized way. Based on the frequency of
>   schedulers operations, categorization is done: local scheduler,
>   global scheduler and enterprise scheduler etc.
382a566
> \end{description}
388,394c572,582
< Next, we present in Figure~\ref{F:graph-algorithm} a classification of resource scheduling algorithms that we found while reviewing a significant set of literature related to cloud computing. We focus in Figure~\ref{F:graph-algorithm} on a relevant subset while focussing on VM placement while considering QoS parameters to guide the scheduling task. An additional classification is based on the type of algorithm used for the scheduling task. Dependent on the locality and large scale of the scheduling task in many cases a deterministic approach is not suitable. Hence, different algorithm categories are listed in Figure~\ref{F:graph-scheduling}.
< 
< %When looking at heuristics~\cite{vivekanandan2011study}
< %we find traditional algorithms such as hill-climbing but also a
< %variety of nature-inspired algorithms. The detail description of
< %existing work in the field of resource scheduling algorithm is done in
< %the next section.
---
> Next we present a short sample on resource scheduling algorithms that
> we found in literature related to cloud computing scheduling as
> showcased in Figure~\ref{F:class-scheduling1221}. We present in the
> figure only a relevant subset of algorithm classifications including
> the distinction between VM placement and QoS parameters based
> algorithms. Dependent on the locality and scope of the scheduling task
> often a deterministic approach is not suitable. When looking at
> heuristics \cite{vivekanandan2011study} we find traditional algorithms
> such as hill-climbing but also a variety of nature inspired
> algorithms. The detail description of existing work in the field of
> resource scheduling algorithm is done in the next section.
398,399d585
< \input{graph-scheduling}
< 
414,416d599
< In this section, we conduct an exemplary but extensive literature
< review of cloud scheduling to confirm the {\bf taxonomy
<  categories}. As part of this review, we present several tables to identify the categories from research and frameworks we reviewed and are related to cloud scheduling. We augmented each table with a first column that is highlighted and refers to the cloud scheduling taxonomy category we identified for this work.
418d600
< To provide an additional guide we introduce several topical sections focusing and grouped the literature based on its main contribution to these groups. However, we avoided a double listing of the research in multiple groups as much as possible to keep the tables small.
420,429c602,616
< As a result we organize this section by scheduling categories related to 
< %
< dynamic scheduling (Section~\ref{sec:dynamic}),
< cloud metric-based scheduling with emphasize (Section~\ref{sec:vm-scheduling}) on 
< energy (Section~\ref{sec:energy}),
< network (Section~\ref{sec:network}),
< cost (Section~\ref{sec:cost}),
< time (Section~\ref{sec:time}),
< reliability (Section~\ref{sec:reliability}),
< security (Section~\ref{sec:security}), 
---
> In this section we conduct an exemplary but extensive literature
> review of cloud scheduling in order to confirm our taxonomy. As part
> of this review, we present a number of tables to compare the reviewed
> research and frameworks related to cloud scheduling.
> 
> We organize this section by scheduling aspects related to 
> 
> dynamic scheduling (Section~\ref{sec:dynamic})
> cloud metric based scheduling with emphasize (Section~\ref{sec:vm-scheduling}) on 
> energy (Section~\ref{sec:energy})
> network (Section~\ref{sec:network})
> cost (Section~\ref{sec:cost})
> time (Section~\ref{sec:time})
> reliability (Section~\ref{sec:reliability})
> security (Section~\ref{sec:security}),
432,434c619,627
< As High-Performance Computing in the cloud is also a service offered by several providers, we also need to be aware of HPC in the cloud (Section~\ref{sec:hpc}) and scientific workflows (Section~\ref{sec:workflow}) that is going to become a field of interest for the scientific community. This is motivated by the fact that transition to cloud services takes place in academic and commercial settings and is explicitly an area of interest for NIST as discussed in the Big Data Reference Architecture Working Group while leveraging activities from the community including the past Grid community.
< 
< In this section, we also review papers with emphasis on scheduling in public clouds (Section~\ref{sec:public}), containers (Section~\ref{sec:container}), function as a service (Section~\ref{sec:faas}) as well as distributed resource providers (Section~\ref{sec:distributed}) which can utilize a service mesh (Section~\ref{sec:mesh}).
---
> We review scheduling needs for 
> HPC in the cloud (Section~\ref{sec:hpc}) and 
> workflows (Section~\ref{sec:workflow}).
> 
> We mention scheduling in public clouds (Section~\ref{sec:public})
> while also looking at containers (Section~\ref{sec:container}),
> function as a service (Section~\ref{sec:faas}) as well as distributed
> resource providers (Section~\ref{sec:distributed}) which can utilize
> service meshs (Section~\ref{sec:mesh}).
442d634
< In literature, we find the distinction between static and dynamic cloud scheduling algorithms. In static scheduling, resources are scheduled once, while in dynamic scheduling updates are applied constantly to find better resource utilization during runtime.
445c637,657
< The latter is often motivated by the need for scalability~\cite{keller2014hierarchical} across and within data centers or increased fault tolerance~\cite{tighe2013distributed}. Association of other metrics into the dynamic scheduling approach is common while including power, network bandwidth and the integration of sophisticated service level agreements~\cite{tighe2013distributed}.
---
> As part of this scheduling task we often also find the distinction
> between static scheduling, where resources are scheduled once,
> ~\cite{jennings2015resource} and dynamic scheduling, which is
> constantly updated during execution to find better resource
> utilization over time. The later is often motivated by need for
> scalability~\cite{keller2014hierarchical} across and within data
> centers or increased fault
> tolerance~\cite{tighe2013distributed}. Association of other metrics
> into the dynamic scheduling approach is common to for example
> integrate power, reduce network bandwidth and enable more
> sophisticated Service level agreements~\cite{tighe2013distributed}. In
> many cases not only the cloud user, but obviously also the cloud
> provider can benefit from dynamic
> scheduling~\cite{tighe2014integrating}. We find that it can be
> beneficial to separate the scheduling task in multiple steps such as
> shown in~\cite{sun2015live}. Here live migration for correlated VMs is
> optimizing on data, compute, and bandwidth. Other cloud metrics such
> as price~\cite{tordsson2012cloud} are also common and will be in more
> detail addressed in Section ~\ref{sec:cost}. Obviously a rich number
> of algorithm can be applied such as shown in
> Section~\ref{sec:heuristic}.
447c659,660
< In many cases, not only the cloud user but also the cloud provider can benefit from dynamic scheduling~\cite{tighe2014integrating}.
---
> Table~\ref{T:dynamic-scheduling} lists a number of efforts related to
> dynamic scheduling while focusing on virtual machine placement.
449d661
< We find that it can be beneficial to separate the scheduling task in multiple steps such as shown in~\cite{sun2015live}. Here, live migration for correlated VMs is optimizing on data, compute, and bandwidth conducted in several steps. Other cloud metrics such as price~\cite{tordsson2012cloud} are also common and will be addressed in Section ~\ref{sec:cost}. To address the scale problem many such algorithms use heuristics as showcased in Section~\ref{sec:heuristic}.
451,454c663
< Table~\ref{T:dynamic-scheduling} lists several efforts related to dynamic scheduling while focusing on virtual machine placement.
< 
< 
< %\input{table-dynamic-placement}
---
> \input{table-dynamic-placement}
460d668
< Due to the complexity of cloud environments, many different metrics are used to guide the scheduling of virtual machines, containers, platforms, tasks, batch jobs, and workflows (see Figure~\ref{F:graph-metrics}). Next, we review examples of literature that integrates such metrics into their scheduling algorithm.
463,464c671,675
< % Figure ~\ref{F:graph-metrics-flower} showcases
< % \input{graph-metrics-flower}
---
> Due to the complexity of cloud environments, many different metrics
> are used to guide the scheduling of virtual machines, containers,
> platforms, tasks, batch jobs and
> workflows. Figure~\ref{F:class-scheduling1221} showcases many
> different metrics that influence their schedule.  
469d679
< Energy consumption is a key issue for cloud providers due to the enormous cost associated with operating hyper-scale and large cloud data centers. By using server consolidation, optimizing operation on physical machines, energy consumption can be reduced in contrast to smaller-scale infrastructure. Also, while using dynamic voltage scaling of processors, energy consumption can be reduced as shown in~\cite{las09dvfs,las10dvfs,calheiros2014energy} by slowing down the services.
471d680
< Various scheduling methods such as to minimize the total makespan~\cite{bessis2013using}, developing dynamic meta-heuristics~\cite{bi2017application}, fractal mathematics~\cite{duan2016energy}, and machine learning clustering and stochastic~\cite{bui2017energy} have been utilized to optimize energy-aware scheduling. Multiple metrics must be included to correlate, for example CPU, RAM, and bandwidth~\cite{zhu2017three}.
473c682,711
< These features, for example, could be utilized to dynamically adapt to peak loads~\cite{duan2016energy} while making processors faster during such periods. Furthermore, migration~\cite{beloglazov2010energy} has naturally an impact on energy cost. Energy cost in multi-cloud and hybrid-cloud data centers in the clouds are discussed in~\cite{quarati2013hybrid,garg2011environment,gai2016dynamic,dabbagh2015energy} while at the same time increasing the cloud provider broker’s revenue.
---
> Energy consumption is a key issue for cloud providers due to the
> enormous cost associated with operating large cloud data center. By
> using server consolidation, optimizing operation on physical machines
> and using dynamic voltage scaling processors, energy consumption can
> be reduced as shown in \cite{las09dvfs,las10dvfs,calheiros2014energy}.
> 
> Various scheduling methods such as minimize the total
> makespan~\cite{bessis2013using}, dynamic
> meta-heuristic~\cite{bi2017application}, fractal
> mathematics~\cite{duan2016energy}, and machine learning clustering and
> stochastic~\cite{bui2016energy} have been utilized.
> 
> 
> It is obvious that multiple metrics must be included the correlate for
> example CPU, RAM and bandwidth~\cite{zhu2017three}. Dynamicity, for
> example, while addressing peak loads~\cite{duan2016energy} or
> migration~\cite{beloglazov2010energy} has naturally also an impact on
> the energy cost. Energy in hybrid and multiple data centers in the
> clouds is used
> in~\cite{quarati2013hybrid,garg2011environment,gai2016dynamic} while
> at the same time increasing the cloud provider brokers revenue. Energy
> consumption in heterogeneous clouds has also been
> considered~\cite{ding2015energy}.  Others create models to predict the
> energy consumption of each virtual machine~\cite{kim2014energy} this
> requires certainly proper monitoring of the underlying server farms in
> the cloud in~\cite{van2012comparison}. Integration of historical or
> previous program executions while recording their energy consumption
> can also be utilized~\cite{hu2010scheduling}. Others focus on
> predicting future resource consumption needs~\cite{dabbagh2015energy}.
> 
476d713
< Others create models to predict the energy consumption of each virtual machine~\cite{kim2014energy}. This requires the ability to properly monitor the underlying server farms in a cloud data center as discussed in~\cite{van2012comparison}. Integration of historical or previous program executions while recording their energy consumption can also be utilized~\cite{hu2010scheduling}. Others focus on predicting future resource consumption needs~\cite{dabbagh2015energy}.
478d714
< A comparison of energy-aware scheduling algorithms in cloud computing is shown in Table~\ref{T:g-a} and \ref{T:g-b}.
480a717,718
> A comparison of energy aware scheduling algorithm in cloud computing is shown in
> Table~\ref{T:g}.
482c720,722
< %\input{table-energy}
---
> 
> 
> \input{table-energy}
486d725
< Clouds promote large-scale network traffic to, from, and within clouds. Thus network-aware scheduling must be considered for scheduling. This not only contains moving data in and out of the cloud data center but may also contain message exchanges between complex distributed applications that run in cloud data centers in a distributed fashion.
488d726
< Minimizing the distance between data providers and data consumers while, for example,replicating data~\cite{www-akamai} can save a significant amount of traffic and has long been applied on the internet as one of its beneficial strategies. Service level agreements (SLA)~\cite{breitgand2012improving} are playing an important role to achieve proper utilization as part of the scheduling effort. Treating the network as shared scarce resource~\cite{rampersaud2016sharing} motivates the development of network-based scheduling algorithms. Also in network-aware scheduling, we find the distinction between static~\cite{biran2012stable} and dynamic scheduling at runtime so we can deal with traffic bursts.
490d727
< A variety of traditional scheduling metrics (see Figure~\ref{F:graph-metrics}) are often used to improve scheduling while considering network traffic. An example is demonstrated in ~\cite{yu2017survivable} to optimize traffic in virtual clusters. Scheduling across multiple layers is especially of benefit for networking~\cite{bi2015sla}. Scheduling of platforms such as Hadoop, offers advantages when networking is integrated~\cite{kondikoppa2012network}. Having access to lower-level infrastructure such as offered by OpenStack, presents opportunities to include Network Function Virtualization (NVF)~\cite{lucrezia2015introducing}.
492c729,754
< Table \ref{T:c} shows examples of network-aware scheduling algorithms in cloud computing.
---
> As clouds project remote large scale resources network traffic to,
> from, and within must be considered for scheduling. This not only
> contains moving data in and out of the compute center or storage, but
> also may contain message exchange between to sometime complex
> distributed applications that run in these cloud data
> centers. Minimizing the distance between data providers and data
> consumers while for example replicating data \cite{www-akamai} can save
> significant amount of traffic and has long been applied in the
> internet as one of its beneficial strategies. Service level agreements
> (SLA)~\cite{breitgand2012improving} are playing an important role to
> achieve proper utilization as part of the scheduling effort. Treating
> the network as shared scarce resource~\cite{rampersaud2016sharing}
> motivates the development of network-based scheduling algorithms.
> Just as in other metric-based scheduling models, we find the
> distingtion between static~\cite{biran2012stable} and dynamic
> scheduling during runtime so we can deal with traffic bursts.
> 
> A variety of resource abstractions (see Figure~\ref{F:class-metric}) are
> applicable also to scheduling as part of the network traffic, such as
> demonstrated by~\cite{yu2017survivable} to optimize traffic in virtual
> clusters.  Scheduling across on multiple layers is especially of
> benefit for networking~\cite{bi2015sla} to minimize across different
> tiers.  Scheduling of platforms such as Hadoop offers naturally
> advantages when networking is integrated~\cite{kondikoppa2012network}.  Having access to lower level infrastructure
> such as offered by OpenStack presents opportunities to include Network
> Function Virtualization (NVF)~\cite{lucrezia2015introducing}
493a756,757
> Table \ref{T:c} shows examples for network aware scheduling algorithms
> in cloud computing.
496c760,761
< %\input{table-network}
---
> 
> \input{table-network}
501d765
< Cost in clouds arises by using the data center facilities. These costs are passed along to the users.
503d766
< Through shared use of the facilities and keeping under-utilization low, clouds can have an advantageous cost-performance ratio compared to on-premise compute and data centers. Costs for such centers include hardware operation, costs such as energy and equipment, as well as, operating costs, such as software licensing and update and personnel costs. Dependent on the hardware and software used, cloud providers offer a tiered cost model that allows users to assess the need for data, speed, and reliability as part of their cost analysis. Other options such as the selection of renewable energy use within the data center in case of energy conscious customers may also play a role.
505c768,798
< Cost aware scheduling has been applied to virtual machines~\cite{yuan2017ttsa}, tasks~\cite{yuan2017temporal,zuo2015multi}, workflows~\cite{arabnejad2015cost,arabnejad2016budget}, as well as high-throughput~\cite{yuan2016cawsac} computing and use of data placement. Revenue maximization~\cite{yuan2018warm} has not only been applied to metrics such as latency~\cite{ghahramani2017toward}, but is also useful via advanced Dynamic Voltage and Frequency Scaling (DVFS)~\cite{las10cloudsched,calheiros2014energy} due to reducing the high energy costs with little performance reduction. This also could be achieved through delayed execution~\cite{bi2016trs} or relaxation of deadlines~\cite{zhang2018dynamic}. Other strategies include the introduction of penalties as part of SLA~\cite{wu2012sla}. Typical resource utilization such as optimizing processor sharing~\cite{lee2012profit} data placements~\cite{lee2012profit}, have been known to decrease cost. Also, dynamic adaptations at run-time allow reduction of cost~\cite{ari2013design}
---
> Cost in clouds arise for using the data center facilities. These costs
> are passed along to the users. Through shared use of the facilities
> and keeping under-utilization low, clouds can have an advantageous
> cost performance in regards to on-premise compute and data
> centers. Costs for such centers include hardware operation cost such
> as energy and equipment, as well as, operating costs such as software
> licensing and update and personnel costs. Dependent on the hardware
> and software used, cloud providers offer a tiered cost model that
> allows users to assess need for data, speed, and reliability as part
> of their cost.  Other options such as renewable energy use of the data
> center in case of energy aware customers may also play a role.
> 
> Cost aware scheduling has been applied to virtual
> machines~\cite{yuan2017ttsa},
> tasks~\cite{yuan2017temporal,zuo2015multi},
> workflows~\cite{arabnejad2015cost,arabnejad2016budget}, as well as
> high-throughput~\cite{yuan2016cawsac} computing.  Revenue
> maximization~\cite{yuan2018warm} has not only been applied to metrics
> such as latency~\cite{ghahramani2017toward}, but is also useful via
> advanced dynamic Voltage and Frequency Scaling
> (DVFS)~\cite{las10cloudsched,calheiros2014energy} due do reducing the
> high energy costs with little performance reduction. This also could
> be achieved through delayed execution~\cite{bi2016trs} or relaxation
> of deadlines~\cite{zhang2018dynamic}.  Other strategies include the
> introduction of penalties as part of SLA~\cite{wu2012sla}. Typical
> resource utilization such as optimizing processor
> sharing~\cite{lee2012profit} data placements~\cite{lee2012profit},
> have known to decrease cost. Obviously also dynamic dynamic
> adaptations at runtime allow reduction of cost~\cite{ari2013design}
> 
> Table \ref{T:e} presents a comparison of cost aware scheduling algorithms.
507d799
< To allow customers to decide the usage of various services including compute, data, function, and platform, most publish extensive cost schemes that can then be integrated into customers scheduling decisions.
509d800
< Table \ref{T:e} presents a comparison of cost-aware scheduling algorithms.
511c802,804
< %\input{table-cost}
---
> \input{table-cost}
> 
> \subsubsection{Time based Scheduling}\label{sec:time}
513d805
< \subsubsection{Time-based Scheduling}\label{sec:time}
516c808,824
< Cloud users have the desire to reduce the time it takes to execute their applications and fulfill deadlines~\cite{arabnejad2017scheduling}. Besides virtual machine and time-based scheduling, it is also important to integrate data-aware scheduling to reduce access time to the data~\cite{vandenbosshe2013}. Historical data~\cite{thomas2015credit} or proxies~\cite{erdil2013autonomic} for execution times help designing time-aware scheduling algorithms. We find algorithms that integrate deadline constraints ~\cite{li2016energy}, completion time~\cite{xu2011job} with fairness, low downtime to improve time for execution~\cite{frincu2014scheduling}, and delay bounds ~\cite{yuan2017time}.
---
> Cloud users have the desire to reduce the time it takes to execute
> their applications. This is often motivated to fulfill
> deadlines~\cite{arabnejad2017scheduling}.
> 
> Besides virtual machine and task scheduling it is also important to
> integrate data-aware scheduling to reduce access time to the
> data~\cite{van2013online}. As already mentioned previously, historical
> data~\cite{thomas2015credit} or proxies~ \cite{erdil2013autonomic}
> about execution times help designing time-aware scheduling algorithms.
> 
> We find algorithms that integrate
> deadline constraint ~\cite{li2016energy}, 
> completion time~\cite{xu2011job} with fairness~\cite{jasso1989theory}, 
> low downtime to improve time for execution~\cite{frincu2014scheduling},
> and 
> delay bounds 
> ~\cite{yuan2018time}.
518c826,827
< Table~\ref{T:f} presents a comparison of time-aware scheduling algorithms.
---
> Table~\ref{T:f} presents a comparison of time aware
> scheduling algorithms.
520c829
< %\input{table-time}
---
> \input{table-time}
526c835,846
< Cloud Users and providers need the guarantee of reliability. Thus, many cloud scheduling efforts address how to increase reliability. Strategies such as replication of data and compute services are common practice. This often comes at a price and increased cost may occur when reliability is of concern. The distributed nature of clouds makes it a formidable challenge to offer reliability. However at the same time, while providing (a) large scale data centers to offer cloud services with (b) highly specialized operating staff and (c) abilities to replicate and migrate workloads to other services, it increases reliability when compared to on-premise data centers. This is often due to the larger efficiency of the cloud data centers regarding the overall cost for its users.
---
> Users and providers need the guarantee of reliability. Thus many
> scheduling efforts integrate how to increase reliability. Strategies
> such as replication of data and compute services are common
> practice. Obviously this comes often at a price and increased cost may
> occur when reliability is concerned. The distributed nature of clouds
> make it a formidable challenge to offer reliability. However at the
> same time while providing large scale data centers to offer cloud
> services with highly specialized operating staff and abilities to
> replicate and migrate workloads to other services increases
> reliability when compared to on-premise data centers due to larger
> efficiency of the cloud data centers in regards to overall cost for
> its users.
530c850,855
< This includes reliability assessment models ~\cite{malik2012reliability}, integration of communication and networks~\cite{jing2015reliability}, increase of resource availability~\cite{latiff2016fault}. Trade-offs between different scheduling metrics such as energy and reliability have also been studied~\cite{tang2016energy}.
---
> This includes reliability assessment models
> ~\cite{malik2012reliability}, integration of communication and
> networks~\cite{jing2015reliability}, increase of resource
> availability~\cite{latiff2016fault}. Trade offs between different
> scheduling metrics such as energy and reliability have also been
> studied~\cite{tang2016energy}.
534d858
< %\input{table-reliability}
536d859
< \subsubsection{Security-based Scheduling}\label{sec:security}
537a861
> \input{table-reliability}
539c863
< Security is a key feature cloud users and providers require for cloud infrastructure to be useful for many applications.
---
> \subsubsection{Security based Scheduling}\label{sec:security}
541d864
< Virtual machine scheduling requires the need for isolation, that can be controlled by security policies~\cite{afoulki2011security}. Isolation can also apply to the incoming and outgoing data~\cite{chejerla2017qos,kashyap2014security}. Risks occurring by inspecting the connections among VMs ~\cite{shetty2016security} can be analyzed and integrated into scheduling strategies. To enable trust between components in the cloud key exchanges have been proposed~\cite{liu2013ccbke}.
543d865
< Multiple possibly contradictory scheduling objectives need to be also considered in many scheduling frameworks.
545c867,885
< An example included the cost it takes to provide security and integrate it adequately in security scheduling frameworks~\cite{kashyap2014security,zeng2015saba,wang2012cloud}. Furthermore, as many edge devices need to interface with cloud services due to their computational and data limitations, privacy-preserving solutions to interface between clouds and mobile and edge devices have been considered ~\cite{bilogrevic2011meetings}.
---
> As mentioned earlier security as a key aspect cloud users and
> providers require in order for cloud infrastructure to be useful for
> many applications.
> 
> Virtual machine scheduling requires the need for isolation, that can
> be controlled by security
> policies~\cite{afoulki2011security}. Isolation can also apply to the
> incoming and outgoing data~\cite{chejerla2017qos,kashyap2014security}.
> Risks occurring by inspecting the connections among VMs
> ~\cite{shetty2016security} can be analyzed and integrated in
> scheduling strategies.  To enable trust between components in the
> cloud background key exchanges have been proposed~\cite{liu2013ccbke}
> Multi-objectives (possibly contradictory) need to be also
> considered. Most often it includes cost vs. security scheduling
> frameworks~\cite{kashyap2014security,zeng2015saba,wang2012cloud}.  As
> many edge devices need to interface with cloud services due to their
> computational and data limitations, privacy-preserving solutions to
> interface between clouds and mobile devices have been considered
> ~\cite{bilogrevic2011meetings}.
547c887,888
< Security-based scheduling algorithms are presented (see Table~\ref{T:i}).
---
> Security based scheduling algorithms are presented (see
> Table~\ref{T:i}). 
551c892
< %\input{table-security}
---
> \input{table-security}
553c894
< \subsection{Heuristic-based Scheduling}\label{sec:heuristic}
---
> \subsection{Heuristic based Scheduling}\label{sec:heuristic}
556d896
< Heuristic methods help to design efficient algorithms in the case where deterministic methods can not be applied. We provide here a small sample of heuristics applied to clouds as found in the literature. This includes particle swarm optimization~\cite{pandey2010particle}, multi-objective genetic algorithm-based ~\cite{mezmaz2011parallel,gkasior2016metaheuristic}, colony optimization with swarm intelligence~\cite{mateos2013aco}, bee colony~\cite{ld2013honey}, artificial neural networks~\cite{kousiouris2011effects}, simulated annealing~\cite{torabzadeh2010cloud}, game-theory~\cite{gkasior2016metaheuristic}, and Game theory by minimizing the Pareto dominance and makespan~\cite{su2013cost}. Other heuristics utilize classical models such as using the critical path in multi-phase scheduling algorithms ~\cite {abrishami2013deadline}. Besides virtual machines we often also find workflows to be the scheduling unit in heuristics~\cite{bousselmi2016qos}.
558,559c898,913
< A comparison of heuristic-based scheduling algorithm is provided in
< Tables~\ref{T:j-a} and \ref{T:j-b}.
---
> Heuristic methods help to design efficient algorithm to fulfill the
> users application requirements. We provide here a small sample of
> different heuristics as found in literature. This includes particle
> swarm optimization~\cite{pandey2010particle}, multi-objective genetic
> algorithm-based ~\cite{mezmaz2011parallel,gkasior2016metaheuristic},
> colony optimization with swarm intelligence~\cite{mateos2013aco}, bee
> colony~\cite{ld2013honey}, artificial neural
> networks~\cite{kousiouris2011effects}, simulated
> annealing~\cite{torabzadeh2010cloud},
> game-theory~\cite{gkasior2016metaheuristic}, and Game theory by
> minimizing the Pareto dominance and makespan~\cite{su2013cost}.  Other
> heuristics utilize classical models such as using the critical path in
> multi-phase scheduling algorithms ~\cite
> {abrishami2013deadline}. Besides virtual machines we often also find
> workflows to be the scheduling unit in
> heuristics~\cite{bousselmi2016qos}.
561c915,916
< %\input{table-heuristic}
---
> A comparison of heuristic methods based scheduling
> algorithm is done in Table~\ref{T:j}.
564d918
< \subsection{ML-based Scheduling}\label{sec:AI}
566c920
< Recently, Machine Learning, and especially Deep Reinforcement Learning (DeepRL)-based approaches have become quite popular due to significant progress in the field. These methods can also be applied to automatically learn to schedule more efficiently in a cloud while it can also adapt to system changes. Various studies have been conducted to analyze the effect of ML-based scheduling approach in the Cloud computing environment. This includes deep reinforcement approach ~\cite{cheng2018drl,mao2018learning}, Q-learning model~\cite{zhang2017energy} and Q network model ~\cite{wang2019multi}. Markov's decision-based approach ~\cite{barrett2013applying} is also studied to handle the uncertainty to provide an optimal decision at the time of scheduling. A comparison of ML-based scheduling algorithms is presented in Table~\ref{T:j1}.
---
> \input{table-heuristic}
568d921
< %\input{table-AI}
570c923
< %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
---
> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
579c932,940
< Next, we review scheduling classifications related to traditional High-Performance Computing (HPC). It is important to recognize, that HPC and its frameworks must not be excluded as part of cloud scheduling review due to its exposure for scientific application in industry and academia. More importantly, HPC is now also offered as one of the supported compute services in public cloud providers. When looking at the services offered and needed we distinguish HPC batch queuing in the cloud, cloud bursting of on-premise HPC tasks, container isolation, on-demand platforms, and bare-metal provisioning.
---
> Next we review scheduling aspects related to traditional High
> Performance Computing (HPC). It is important to recognize, that HPC
> and its frameworks must not be excluded as part of cloud scheduling
> due to its exposure for scientific application in industry and
> academia. More importantly HPC is now also offered as one of the supported
> compute services in public cloud providers. When looking at the services offered
> and needed we distinguish HPC batch queuing in the cloud, cloud
> bursting of on premise HPC tasks, container isolation, on demand
> platforms and bare metal provisioning.
583,591c944,991
< \item[HPC Batch Queuing in the Cloud.] Cloud providers offer specialized high-performance super-computing systems to customers with computation needs that can only be fulfilled by large scale specialized hardware. Grand challenge problems are often motivators for such hardware. In the industry, we, for example, find computational fluid dynamics, and modeling of biochemical processes as one of its drivers. Example offerings for HPC in the cloud are provided by AWS \cite{www-aws-hpc}, Azure~\cite{www-azure-hpc}, Google~\cite{www-google-hpc}, but also other less prominent clouds such as Penguin Computing HPC in the cloud~\cite{PODHPCCloud2019}, and SabalCore~\cite{Sabalcore2019}.
<  
< \item[Cloud Bursting of On-Premise HPC tasks.] The on-premise HPC systems are often over-utilized and thus the situation of resource starvation motivates the provider to gain additional resources in the cloud. For this reason, many batch queuing system allows the integration of cloud resources in such a fashion that task and workflows may be executed in the cloud through the integration of commercial or on-premise cloud resources. In this case, the term cloud bursting is used \cite{CloudBursting2019,BurstingHPC2019}. Examples for the integration in prominent HPC scheduling includes Slurm~\cite{www-slurm}, Univa Grid Engine~\cite{www-univa}, PBSpro~\cite{www-pbs-manual}, LSF~\cite{www-lsf}, Moab~\cite{www-moab}.
< 
< \item[Container Isolation.] Due to the usage of queuing systems it is also possible to provide in part an improved container framework while executing containerized tasks as part of the queuing system. An example would be to utilize all cores in a compute-server that is allocated with a queuing system processor. This feature can be integrated into many queuing systems while using Singularity~\cite{www-singularity}.
< 
< \item[On Demand Platforms.] Resource starvation in academic clouds and supercomputing centers motivate also the ability to run platforms that would typically run also in the cloud but provide an alternative if run locally in the existing HPC centers. A good example is Hadoop that can be run through myhadoop~\cite{krishnan2011myhadoop} in HPC centers~\cite{SDSC2019}.
< 
< \item[Bare Metal Provisioning.] In other cases it may be better to provide bare-metal provisioning capabilities in case existing platform or cloud abstraction may not be sufficient. Academic efforts such as FutureGrid~\cite{las12fg-bookchapter} now followed by Comet~\cite{las-comet} and Chameleon Cloud~\cite{Chameleoncloud2019} are good examples for it. Commercial efforts in this regard include OpenStack Ironic ~\cite{OpenstackIronic2019}, IBM~\cite{IBMBareMetal2019}, AWS ~\cite{AWS2019} and Rackspace~\cite{Rackspace2019}.
---
> \item[HPC Batch Queuing in the Cloud.] These are specialized
>   high-performance super-computing systems that are offered to
>   customers with computation needs that can only be fulfilled by large
>   scale specialized hardware. Grand challenge problems are often
>   motivators for such hardware. In industry we for example find
>   computational fluid dynamics, and modeling of biochemical processes
>   as one of its user communities. Example offerings for HPC in AWS
>   \cite{www-aws-hpc}, Azure \cite{www-azure-hpc}, Google \cite{www-google-hpc},
>   but also other less prominent clouds such as Penguin Computing HPC
>   in the cloud~\cite{PODHPCCloud2019}, and
>   SabalCore~\cite{Sabalcore2019}.
>   
> \item[Cloud Bursting of On Premise HPC tasks.] The HPC systems are
>   often over-utilized and thus the situation of resource starvation is
>   to be considered. For this reason many batch queuing system allow
>   the integration of cloud resources in such a fashion that task and
>   workflows may be executed in the cloud through the integration of
>   commercial or on premise cloud resources. In this case the term
>   cloud bursting is used
>   \cite{CloudBursting2019,BurstingHPC2019}. Example for the
>   integration in prominent HPC scheduling includes Slurm~\cite{www-slurm},
>   Univa Grid Engine~\cite{www-univa}, PBSpro~\cite{www-pbs-manual}, LSF~\cite{www-lsf-job-dep},
>   Moab~\cite{www-moab-job-dep}.
> 
> \item[Container Isolation.] Due to the usage of queuing systems it is
>   also possible to provide in part an improved container security
>   framework, while executing containerized tasks as part of the
>   queuing system. An example would be to utilize all cores in a
>   compute server that is allocated with a queuing system
>   processor. This feature can be integrated into many queuing systems
>   while using Singularity~\cite{www-singularity}.
> 
> \item[On Demand Platforms.] Resource starvation in academic cloud and
>   super computing centers motivate also the ability to run platforms
>   that would typically run also in the cloud but provide a cheaper
>   alternative if run locally in the existing HPC centers. A good
>   example is Hadoop that can be run through
>   myhadoop~\cite{krishnan2011myhadoop} in HPC centers~\cite{SDSC2019}.
> 
> \item[Bare Metal Provisioning.] In other cases it may be better to
>   provide bare metal provisioning capabilities in case the
>   existing platform or cloud abstraction may not be sufficient.
>   Academic efforts such as Futuregrid~\cite{fox2013futuregrid} now
>   followed by Comet~\cite{las-comet} and Chaleleoncloud~\cite{Chameleoncloud2019} 
>   are good examples for it. Commercial
>   efforts in this regard include OpenStack Ironic
>  ~\cite{OpenstackIronic2019}, IBM~\cite{IBMBareMetal2019}, AWS
>  ~\cite{AWS2019} and Rackspace~\cite{Rackspace2019}.
595,596d994
< Table ~\ref{T:l} presents a selected comparison of the different batch resource management systems.
<  
598c996
< %\input{table-batch}
---
> \input{table-batch}
602c1000
< % Workflow
---
> % WORkflow
606d1003
< 
610,616d1006
< In the previous sections, we already pointed out several workflow related scheduling algorithms while using specific metrics to conduct the scheduling. Also, we can integrate virtual machines, containers, and tasks. The main intent of the cloud workflow is to automate repeated tasks in a reliable way. 
< 
< It is important to recognize that previous academic task-based workflow engines can easily be modified to be integrated into the scheduling needs of clouds. This includes traditionally, workflow schedulers distinguished by DAG and non-DAG scheduling strategies~\cite{deelman2005pegasus,deelman2004pegasus,thain2005distributed,las-infogram,las-karajan,las-cogkit-1,las06-workflow-book}. This includes the ability to integrate virtual machines, containers, and functions into task abstractions as demonstrated for example in~\cite{www-pegasus-container,www-cloudmesh-manual} recently. A very recent effort includes the development of cloudmesh cloud that provides abstractions to interface with VMs, FaaS, PaaS, and containers~\cite{www-cloudmesh-manual}. This effort also includes needed information such as the cost of virtual machines that can be leveraged into the use and development of cost-based scheduling algorithms.
< 
< Furthermore, It is often an overlooked fact that as part of the scheduling within existing HPC batch queuing systems workflow abstractions exist that could be utilized to integrate cloud scheduling tasks. Such queuing systems include~\cite{www-lsf,www-moab,www-univa-GE-manual,www-pbs-manual}. These systems can naturally be utilized to facilitate cloud bursting, as well as the scheduling of IaaS, PaaS, and FaaS, as is successfully demonstrated on Comet~\cite{las-comet} at Sandiego Super Computing Center.
< 
< % Scientific applications such as bio-informatics have introduced not only workflow systems, but also promoted graphical workflow design tools to create dependency graphs the are executed by workflow schedulers~\cite{oinn2004taverna,tan2010comparison,yu2005taxonomy}.
618d1007
< Additionally, we see another important aspect, where cloud providers host their workflow services in the cloud. This includes efforts such as Nintex ~\cite{www-nintex-wf}, Amazon Simple Workflow (ASW), Hadoop streaming~\cite{www-hadoop-streaming} for map-reduce workflows are used. For example, ASW intends to provide support to build, run, and scale background jobs that have parallel or sequential steps. The main intent of Spark streaming ~\cite{www-spark-streaming} is to provide scalable, high-throughput and fault-tolerant stream processing of live data streams. Argo ~\cite{www-argo-wf}, a container-native workflow engine is used for orchestrating parallel jobs on Kubernetes. This will become a rich area of research as workflows need to utilize resources and efficient workflow schedulers to utilize cloud resources is an important goal.
620c1009,1026
< Other trends are discussed in \cite{data-aware-scheduling,workflow-scheduling-cloud,}.
---
> In the previous sections we already pointed out several workflow
> related scheduling algorithms while using specific metrics to conduct
> the scheduling. In addition we can integrate virtual machines,
> containers, and tasks.
> 
> Workflow schedulers are often distinguished by
> DAG~\cite{deelman2005pegasus,deelman2004pegasus,thain2005distributed}
> and non-DAG scheduling while some support
> both~\cite{las-karajan,las-cogkit-1,las06-workflow-book}. Even prior
> to the official FaaS frameworks existed that focused on the execution
> of functions~\cite{las-infogram}.  Scientific applications such as
> bio-informatics have introduced not only workflow systems, but also
> promoted graphical workflow design tools to create dependency graphs
> the are executed by workflow
> schedulers~\cite{oinn2004taverna,tan2010comparison}.
>   
> In addition to our findings, in~\cite{yu2005taxonomy} workflows are
> also organized by design, scheduling and data movement abilities.
621a1028,1033
> It is often an overlooked fact that existing HPC batch queuing systems
> contain features for job dependency management. In many cases these
> features can be used to accommodate the users needs for scheduling
> workflows onto the same hardware or in some cases clusters that are
> managed through the same queuing
> system~\cite{www-lsf-job-dep,www-moab-job-dep,www-univa-GE-manual,www-pbs-manual}.
628,632d1039
< Next, we compare scheduling methods and needs offered in public cloud service providers. This includes AWS, Azure, Google, Rackspace, but also academic clouds such as FutureGrid and FutureSystems Comet, Jetstream, and Chameleon Cloud.
< 
< It is important to recognize that today public cloud providers offer not only virtual machines to the users, but a large variety of compute, data, and analytical services. Some of them may even use bare metal while others are having heightened security demands, to, for example, fulfill heath care or government isolation needs as part of the infrastructure. All these issues naturally influence the scheduling efforts which need to be addressed by the provider. In many cases, we do not find sufficient information on how such scheduling is conducted due to security and company secrets.
< 
< However, we find metrics that users can utilize to formulate their strategies as we have introduced in the previous section if such metrics are communicated to the users. This typically includes cost and allows to leverage for example virtual machine with reliability constraints such as AWS spot pricing compared to regular pricing~\cite{AmazonEC22015}. Cost also motivates users to suspend the usage of VMs instead of running them without concern. This has happened to the authors of this paper, where in a class a student, refused to shut down experimental virtual machines and within two weeks consumed thousands of compute hours on an academic cloud, while the actual calculation was irrelevant.
634d1040
< One of the schedulers provided by public clouds are job and instance schedulers that promote start and stop times for the resources used~\cite{AWSIns2019,AzureSch2019,Rackspace2016,GoogleAppEngine2018}. Such schedulers can integrate functions, data and compute instances. More sophisticated schedulers can switch workloads between cloud data centers~\cite{MicrosoftAzure2014}.
636,640c1042,1095
< In ~\cite{Rackspace2016} cloud load-balancer, round-robin and least connections-based algorithms are commonly used so that workload could be distributed equally on all resources. As one of the original tasks of clouds was hosting of Web services under traffic load, public clouds include strategies that scale up and down the services-based on such loads and allocate resources dynamically.
< 
< Other providers have focused on making use of multi-cloud virtual machine placements while offering optimization strategies for workflows~\cite{CloudSigma2016} including a detailed analysis of cost metrics~\cite{Cloudmetrics2019}
< 
< Other efforts such as~\cite{las12fg-bookchapter} have early on uniquely focused on scheduling bare-metal resources between the use of HPC and clouds while running HPC queuing systems on the same resources as cloud resources. Dynamic provisioning allowed resources to be provisioned to the one or the other by demand. In~\cite{las-comet} the re-provisioning is even done with the help of a traditional batch queuing system enabling a sophisticated scheduling approach
---
> Next, we compare scheduling methods and needs offered in public cloud
> service providers are presented. This includes AWS, Azure, Google,
> Rackspace, but also academic clouds such as FutureGrid and
> FutureSystems Comet, Jetstream, and Chameleon Cloud.
> 
> It is important to recognize that today public cloud providers offer
> not only virtual machines to the users, but a large variety of
> compute, data, and analytical services. Some of them may even use bare
> metal while others are having heightened security demands to for
> example fulfill heath care or government isolation needs as part of
> the infrastructure. All these aspects naturally influence the
> scheduling efforts which need to be addressed by the provider. In many
> cases we do not find some of the information on how such scheduling is
> conducted due to security and company secrets.
> 
> However we find metrics that users can utilize to formulate their own
> strategies as we have introduced in the previous section if such
> metrics are communicated to the users. This typically includes cost
> and allows to leverage for example virtual machine with reliability
> constraints such as AWS spot pricing compared to regular
> pricing~\cite{AmazonEC22015}.  Cost also motivates users to suspend
> usage of VMs instead of running them without concern. This has
> happened to the authors of this paper, where in a class a student,
> refused to shutdown experimental virtual machines and within two weeks
> consumed thousands of compute hours on an academic cloud, while the
> actual calculation was irrelevant.
> 
> One of the schedulers provided by public clouds are job and instance
> schedulers that promote start and stop times for the resources
> used~\cite{AWSIns2019,AzureSch2019,Rackspace2016,GoogleAppEngine2018}. Such
> schedulers can integrate functions, data and compute instances. More
> sophisticated schedulers can switch workloads between cloud data
> centers~\cite{MicrosoftAzure2014}.
> 
> In ~\cite{Rackspace2016} cloud load-balancer, round robin and least
> connections based algorithms are commonly used so that workload could
> be distributed equally on all resources.  As one of the original tasks
> of clouds was hosting of Web services under traffic load. public
> clouds include strategies the scale up and down the services based on
> such loads and allocate dynamically through a scheduler resources to
> fulfill this demand.
> 
> Other providers have focused on making use of multicloud virtual
> machine placement possible while offering optimization strategies for
> workflows~\cite{CloudSigma2016} including detailed analysis of cost
> metrics \cite{Cloudmetrics2019}
> 
> Other efforts such as~\cite{las12fg-bookchapter,fox2013futuregrid}
> have early on uniquely focused on scheduling bare metal resources
> between the use of HPC and clouds, while running HPC queuing systems
> on the same resources as cloud resources. Dynamic provisioning allowed
> resources to be provisioned to the one or the other by
> demand. In~\cite{las-comet} the re-provisioning is even done with the
> help of a traditional batch queuing system.
646c1101
< %\input{table-iaas}
---
> \input{table-iaas}
653,657c1108,1125
< Container schedulers provide mechanisms to fine-tune the selection processes of containers onto distributed resources~\cite{Containers2018,de2018distributed}. Typically a default scheduling policy is provided. Policies might place new services on hosts with the fewest currently active services.
< 
< Based on the Y-diagram we need to distinguish two different services. First, scheduling on the same server and second scheduling on several servers that are treated typically as one abstract cloud resource
< 
< For the first scheduling task, we need to consider data management to efficiently utilize the memory hierarchy, but also, for example, execution deadlines or privacy concerns to organize the computation tasks as required. In the distributed case we also need to integrate communication-related issues. We focus next on the distributed frameworks in more detail we focus on Docker Swarm, Kubernetes, Singularity, and Mesos.
---
> Container schedulers provide mechanisms to fine-tune the selection
> processes of containers onto distributed
> resources~\cite{Containers2018,de2018distributed}. Typically a default
> default scheduling policy is provided. Policies might place new
> services on hosts with the fewest currently active services.
> 
> Based on our Y-diagram we need to distinguish 2 different
> services. First, scheduling on the same server and second scheduling
> on a number of servers that are treated typically as one abstract
> resource.
> 
> For the first scheduling task we need to consider data management to
> efficiently utilize the memory hierarchy, but also for example
> execution deadlines or privacy concerns to organize the computation
> tasks as required. In the distributed case we also need to integrate
> communication related aspects. We focus next on the distributed
> frameworks in more detail we focus on Docker Swarm, Kubernetes,
> Singularity, and Mesos.
662,668c1130,1136
< \item[Docker Swarm.] Docker Swarm is a clustering and scheduling tool for Docker containers~\cite{Dockerswarmengine2018} across compute servers. In a docker swarm, we distinguish manager nodes and worker nodes. The manager uses load balancing to place the containers onto the worker nodes. Once a task is placed on a server it is executed there. Docker swarm uses a single scheduling strategy~\cite{Dockerswarm2018}.
< 
< 
< 
< \item[Kubernetes.] Kubernetes is an open-source orchestrator developed by Google for automating container management and deployment~\cite{Kubernates2018}. The basic deployable object is a Pod that consists of one or more containers running in a shared context. An API is used to declare policies and scalability constraints. The Kubernetes scheduler is topology-aware and workload aware which can be integrated into the policy constraints to expose availability, performance, and capacity. Auto-scaling, load balancing, and secrets management are also provided by Kubernetes.
< 
< \item[Singularity.] Singularity can be using a variety of container frameworks as backend. It allows the use of containers without being a superuser. Due to this, singularity is a popular choice for running containers on traditional HPC systems~\cite{www-singularity}. Due to this scheduling can be supported directly by the under-laying queuing system.
---
> \item[Docker Swarm.] Docker Swarm is a clustering and scheduling
> tool for Docker containers \cite{Dockerswarmengine2018} across compute
> servers. In a docker swarm we distinguish manager nodes and worker
> nodes. The manager uses load balancing to place the containers onto
> the worker nodes. Once a task is placed on a server it is executed
> there.  Docker swarm uses a single scheduling
> strategy~\cite{Dockerswarm2018}.
671,674d1138
< \item[Mesos.] Mesos~\cite{hindman2011mesos,Mesos2018} provides an API for resource management and scheduling in data centers. Mesos abstracts CPU, memory, storage, and other compute resources. It integrates fault-tolerance. Mesos provides a thin resource sharing layer that helps to furnish fine-grained sharing by providing common interfaces among different cluster frameworks. Its goal is improved utilization, respond quickly to workload changes, by maintaining a system's capability in terms of scalability and robustness.
< 
< 
< \item[Community Efforts.] Many community efforts to improve container scheduling are conducted. This includes, for example, the use of genetic algorithm~\cite{guerrero2018genetic}, container, and host selection policies for cloud deployment models~\cite{hanafy2017novel} with SLA's, the characterization of applications~\cite{medel2017client} scheduling of virtual clusters~\cite{dziurzanskivalue}, and migration~\cite{Flocker2018}, and systems integrating multiple schedulers such as Nomad which offer service scheduler, batch scheduler, and a systems scheduler while focusing on the support of long-running jobs~\cite{Nomad2018}.
675a1140,1177
> \item[Kubernetes.] Kubernetes is an open-source orchestrator
> developed by Google for automating container management and
> deployment~\cite{Kubernates2018}. The basic deploy-able object is a
> Pod which consists of one or more containers running in a shared
> context. An API is used to declare policies and scalability
> constraints. The Kubernetes scheduler is topology aware and workload
> aware which can be integrated into the policy policy constraints to
> expose availability, performance and capacity. Auto-scaling, load
> balancing and secrets managements are also provided by Kubernetes.
> 
> \item[Singularity.] Singularity can be using a variety of
> container frameworks as backend. It allows the use of containers
> without being superuser. Due to this, singularity is a popular choice
> for running containers on traditional HPC
> systems~\cite{www-singularity}. Due to this scheduling can be
> supported directly by the under-laying queuing system.
> 
> 
> \item[Mesos.] Mesos~\cite{hindman2011mesos,Mesos2018} provides an
> API for resource management and scheduling in data centers. Mesos
> abstracts CPU, memory, storage, and other compute resources. It
> integrates fault-tolerance. Mesos provides a thin resource sharing
> layer that helps to furnish fine-grained sharing by providing common
> interfaces among different cluster frameworks. It's goal is improved
> utilization, respond quickly to workload changes, by maintaining
> system's capability in terms of scalability and robustness.
> 
> 
> \item[Community Efforts.] Many community efforts to improve
> container scheduling are conducted. This includes for example the use
> of genetic algorithm~\cite{guerrero2018genetic}, container and host
> selection policies for cloud deployment models \cite{hanafy2017novel}
> with SLA's, the characterization of
> applications~\cite{medel2017client} scheduling of virtual
> clusters~\cite{dziurzanskivalue}, and migration~\cite{Flocker2018},
> and systems integrating multiple schedulers such as Nomad which offer
> service scheduler, batch scheduler and a systems scheduler while
> focusing on the support of long running jobs \cite{Nomad2018}.  
677,679c1179
< 
< 
< Table ~\ref{T:z} shows the comparison of existing work related to container scheduling.
---
> Table ~\ref{T:z} shows the comparison of existing work related to scheduling. 
684c1184
< %\input{table-container}
---
> \input{table-container}
694,698d1193
< To further improve scheduling on cloud resources, the concept of Function as a Services was introduced. It allows the invocation of small functions with limited resource constraints on servers~\cite{lasbook}. For example, a minimum execution time per request is five minutes provided by AWS lambda and Azure functions~\cite{ServerlessComputing2018}. It supports managed user-defined functions on highly available infrastructure in a unified fashion~\cite{nastic2017serverless}. This also allows the scheduling of workflows comprised out of functions~\cite{alqaryouti2018serverless}. In~\cite{fox2017status} we discuss the status of serverless computing and function as a service in Industry and research. Serverless computing is considered the backend for running FaaS at runtime. System allocation and other resource management activities are provided by the backend. Thus the users have not to worry about activities conducted by the server. Hence, the name serverless computing. Through the use of FaaS and serverless computing, cost can be reduced by more efficiently scheduling smaller tasks on resources.
< 
< Several FaaS frameworks exist that can be used on public clouds but also self-hosted clouds or network of workstations.
< 
< Scheduling in FaaS is provided by triggers. Such triggers offer a publish-subscribe model in which events are conducted, once the trigger is fired. This includes triggers for time, data, and executions. Time-based scheduling is supported by cron. These frameworks are supported by all major public clouds including AWS lambda~\cite{AWSlambda2018}, Google cloud functions~\cite{GoogleCF2018}, Azure Function~\cite{Azure2018}. This can also be combined with simple workflow scheduling as introduced in pipelines as part of, for example as used in Jenkins~\cite{www-jenkins-pipeline}.
700d1194
< Other open-source frameworks such as Apache OpenWhisk~\cite{OpenWhisk2018} allow users to install FaaS services on their own infrastructure.
702c1196,1228
< An important aspect of scheduling in FaaS is that the deployment of the function itself does cost time. At times the start-up time for the function is substantial. In such cases, workflows can be leveraged to assure that before the function is executed a cache is set up in which the function is deployed~\cite{faas-package-cache}. Thus it is important to assure that deployment times are minimized and when multiple function calls are conducted the deployment is done only once.
---
> To further improve scheduling on cloud resources, the concept of
> function as a services was introduced.  It allows the invocation of
> small functions with limited resource constraints on
> servers~\cite{lasbook}. For example a minimum execution time per
> request is five minutes provided by AWS lambda and azure
> functions~\cite{ServerlessComputing2018}. It supports manage user
> defined functions on highly available infrastructure in an unified
> manner~\cite{nastic2017serverless}. This also allows the scheduling of
> workflows comprised out of
> functions~\cite{alqaryouti2018serverless}. In~\cite{fox2017status} we
> discuss the status of serverless computing and function as a service
> in Industry and research.  Serverless computing is considered the
> backend for running FaaS at runtime. System allocation and other
> resource management activities are provided by the backend. Thus the
> users has not to worry about activities conducted by the
> server. Hence, the name serverless computing. Through the use of FaaS
> and serverless computing cost can be reduced by more efficiently
> scheduling smaller tasks on resources.
> 
> A number of FaaS frameworks exist that can be used on public clouds
> but also self hosted clouds or network of workstations.
> 
> Scheduling in FaaS is provided by triggers. Such triggers offer a
> publish subscribe model in which events are conducted, once the
> trigger is fired. This includes triggers for time, data, and
> executions. Time based scheduling is supported by cron.  These
> frameworks are supported by all major public clouds including AWS
> lambda~\cite{AWSlambda2018}, Google cloud
> functions~\cite{GoogleCF2018}, Azure Function~\cite{Azure2018}
> 
> Other open source frameworks such as Apache
> OpenWhisk~\cite{OpenWhisk2018} allow users to install FaaS services on
> their own infrastructure.
704,705d1229
< In production Clouds such as AWS we also use the term cold, warm, and hot for classifying 
< the preparation of the software to execute a function repeatedly. In these environments a function becomes cold after a particular time, meaning that it needs to restart its functional requirements before the actual function can be executed.
707c1231
< \subsection{Scheduling Among Distributed Resources and Providers}
---
> \subsection{Scheduling among Distributed Resources and Providers}
711d1234
< Users may have the desire to not only use services on one cloud but multiple clouds. This is motivated largely by avoiding vendor lock-in, unique service offerings, or combining services from different vendors.
713,715d1235
< Such scheduling efforts can be as simple as switching the cloud provider such as promoted in Cloudmesh~\cite{von2014accessing}. Other efforts such as Eagle, provide a hybrid data center scheduler for data-parallel programs~\cite{delgado2016job}; Hopper~\cite{ren2015hopper}, a job scheduler that trades off existing and speculated job scheduling decisions; Tetris ~\cite{grandl2015multi}, a cluster scheduler that aims to match multi-resource task requirements with resource availability; Fawkes~\cite{ghit2014balanced} a multi-cluster systems for map-reduce; Omega~\cite{schwarzkopf2013omega} with optimistic concurrency control; OurGrid~\cite{andrade2003ourgrid,cirne2006labs} for worldwide computing platform with isolated environments; Sparrow~\cite{ousterhout2013sparrow} and fine-grained task scheduling scheduler.
<  
< We can also find more prominent schedulers such a contains Apache's Hadoop YARN~\cite{vavilapalli2013apache} which acts as a resource management system to for example schedule Hadoop distributed processing framework considering QoS, scalability, higher efficiency, and fair resource usage.
717d1236
< We contrast different resource management systems, used for maintaining resources in distributed environments such as Clouds (see Table \ref{T:distr-cloud}).
719c1238,1241
< %\input{table-distr-cloud}
---
> Users may have the desire to not only use services on one cloud but on
> multiple clouds. This is motivated largely by avoiding vendor lock-in,
> unique service offerings, or combining services from different
> vendors.
721,725c1243
< \subsection{Data-based Scheduling}
< 
< In cloud applications, it is not only important to integrate compute services into the scheduling decision, but also data. Here we distinguish typically two models. Bring the data to compute services or bring the compute services to the data. A comprehensive scheduling algorithm must be deciding which of these approaches is most amenable to the problem. Metrics such as bandwidth, latency, but also the size of the data are to be integrated into such decision processes. 
< 
< Just as with compute services other metrics can be integrated into the scheduling algorithm to optimize the strategy. Most recently the introduction of Function as a Service is providing the potential for further improving the utilization of data services while concurrently scheduling many functions to operate on data in parallel, streaming, and high-performance~\cite{twister2}. 
---
> \input{table-distr-cloud}
726a1245,1266
> Such efforts contain Eagle, a hybrid data center scheduler for
> data-parallel programs~\cite{delgado2016job};
> Hopper~\cite{ren2015hopper}, a job scheduler that trades off existing
> and speculated job scheduling decisions; Tetris
> ~\cite{grandl2015multi}, a cluster scheduler that aims to match
> multi-resource task requirements with resource availability;
> Fawkes~\cite{ghit2014balanced} a multi-cluster systems for map-reduce;
> Omega~\cite{schwarzkopf2013omega} with optimistic concurrency control;
> OurGrid~\cite{andrade2003ourgrid,cirne2006labs} for worldwide
> computing platform with isolated environments;
> Sparrow~\cite{ousterhout2013sparrow} and fine-grained task scheduling
> scheduler.
>   
> We can also find more prominent schedulers such a contains Apache's
>   Hadoop YARN~\cite{vavilapalli2013apache} which acts as a resource
>   management systems to for example schedule Hadoop distributed
>   processing framework considering QoS, scalability, higher efficiency
>   and fair resource usage.
> 
> We contrast different resource management systems, used for
> maintaining resources in distributed environments such as Clouds (see
> Table \label{T:distr-cloud}).
728c1268
< \subsection{Service Mashups} 
---
> \subsection{Service Meshups} 
731,737c1271,1279
< To support scheduling across clouds and services, service mashups can be used. This includes long-standing efforts such as Cloudmesh~\cite{von2014accessing}, which targets the creation of reproducible environments to easily manage virtual machines, bare-metal provisioned operating systems, platform deployments and more recently data services in a multi-cloud environment. It is a goal to integrate custom schedulers in such service mashups. Another example is Terraform~\cite{www-terraform} which focuses on reproducible environments.
< 
< \subsection{Simulators}
< \label{sec:simulators}
< 
< When developing scheduling frameworks it is important to evaluate them before use. For this reason, it is useful to simulate cloud environments before deploying them in real-time. As many researchers do not have access to large scale clouds this is often the only application we find for many papers and thus they stay more theoretical in nature. Efforts such as FutureGrid~\cite{las12fg-bookchapter} that provided the earliest multi-cloud environment,
< ChameleonCloud~\cite{Chameleoncloud2019}, and CloudLab~\cite{www-cloudlab} did and do provide environments in which clouds can be deployed and algorithms can be tested.
---
> To support scheduling across clouds and services, service mashups can
>   be used. This includes long standing efforts such as Cloudmesh,
>   which targets the creation of reproducable environments to easily
>   manage virtual machines, bare metal provisioned operating systems,
>   platform deployments and more recently data services in a
>   multi-cloud environment. It is a goal to integrate custom schedulers
>   in such service mashups. Another example is
>   Terraform~\cite{www-terraform} which focuses on reproducible
>   environments.
739,741d1280
< For simulation-based efforts we find that the following can be of help TPC-H~\cite{www-tpc-h}, BigDataBench~\cite{bigdatabench}, Google
< Cluster traces~\cite{www-google-cluster-traces}, DCSim~\cite{www-dcsim}
< and CloudSim~\cite{www-cloudsim}.
743,747d1281
< \subsection{Selection of a Scheduling Framework}
< 
< As we can see from the discussion in this paper, the number of parameters and considerations to select a scheduling algorithm fine-tuned for the particular cloud is a significant challenge.  Not only do we need to identify the parameters that will lead to a better schedule, but we also need to understand the scale and scope of compute, data, network and also other inputs that may not be available to users but only available to cloud providers. Hence as much information as possible must be exposed between the different layers so that smart scheduling algorithms can be developed and address quality of service assertions in time and space.
< 
< One thing we observed is that some papers do present a fair number of simulations, but a precise formula for communicating the complexity of the algorithm may not be provided. We believe in a future research activity such a complexity analysis that needs to be conducted while including parameters use to determine the could needs. Furthermore, an automated simulation framework that predicts the performance of the scheduler based on its input can be derived. The need for this is motivated by the potential time constraints we have. Certainly,  we do not have the luxury to wait for a scheduling decision if it takes longer to derive it than the actual calculation or data analysis takes. Furthermore, we envision a REST service based on the NIST Big Data Reference Architecture principles that would allow us to stage an analytics service in which multiple competing algorithms can be evaluated to make the selection process of the scheduler more transparent. Not only would this framework be able to present the complexity but also instantiate or look up a benchmark for the particular scheduling problem formulated.
761c1295
< In this section, we summarize some of the lessons we learned from our activities.
---
> In this section we summarize some of the lessons we learned from our activities.
765,783c1299,1357
< \item[More than VMs.] Due to the shift and enhancement of clouds from VM to containers and FaaS, we must consider also new scheduling strategies as motivated by new cloud compute service offerings utilizing them. This offers several opportunities for research activities.
< 
< \item[Energy.] Energy costs for data centers are enormous and this plays a significant role for providers, but also for users to which energy costs are passed along. Not only good scheduling algorithms are needed, but the design of the data center close to cheap energy is an important issue.
< 
< \item[Y-Diagram.] The Y-diagram promotes scheduling across scale and models. This allows creating a hierarchy of interfacing scheduling approaches for integrated and layered scheduling between resources at different scales.
< 
< \item[Multi-Metric and Multi-Objectivity.] Scheduling algorithms must use multiple metrics and multiple objectives to provide effective scheduling decisions. In many cases, contradictory scheduling goals such as reliability vs cost are to be considered.
< 
< \item[Policy driven.] Due to multi-metric and multi-objective scheduling goals modern schedulers will expose them through policies to users and providers.
< 
< \item[Iterative Optimization in Layers.] Due to the complexity of the scheduling efforts motivated by out Y-diagram, a layered scheduling approach seems appropriate.
< 
< \item[Security and Privacy.] We need to deal more stringently with security and privacy as part of scheduling needs.
< 
< \item[Fault tolerance and Risk Analysis.] As part of the policy-driven service level agreements with the cloud providers schedule must include the ability to integrate fault tolerance while leveraging risk analysis.
< 
< \item[Traditional Scheduling.] Naturally we need to deal with scheduling with traditional issues such as load balancing, congestion, and service spikes. However, they are amplified by formidable resource management issues in hyper-scale data centers.
< 
< 
---
> \item[More than VM scheduling.] Due to the shift and enhancement of
> clouds from VM to containers and FaaS, we must consider also new
> scheduling strategies as motivated by thes new cloud compute
> offerings.
> 
> \item[Integration for data.] Big Data and management of data in
> general motivate the integration of data resources as firs class
> activity within scheduling.
> 
> \item[Energy.] Energy costs for data centers are enormous and this
> play a significant roll for providers, but also for users to which
> energy cost are passed along. Not only good scheduling algorithms are
> needed, but the design of the data center close to cheap energy is an
> important aspect.
> 
> \item[Y-Diagram.] Our Y-diagram promotes scheduling across scale and
> models. This allows to create hierarchy of interfacing scheduling
> approaches for integrated and layered scheduling between resources at
> different scales.
> 
> \item[Multi-Metric and Multi-Objectivity.] Scheduling algorithms must
> use multiple metrics and multiple objectives to provide effective
> scheduling decisions. In many cases contradictory scheduling goals
> such as reliability vs cost are to be considered.
> 
> \item[Policy driven.] Due to multi-metric and multi-objective
> scheduling goals modern schedulers will expose them through policies
> to users and providers.
> 
> 
> \item[Iterative Optimization in Layers.] Due to the complexity of the
> scheduling efforts motivated by out Y-diagarm, a layered scheduling
> approach seems appropriate.
> 
> \item[Analytics Services.] While FaaS provide the ability to schedule
> resource restricted functions the next level of schedulers will
> address Analytics as a Service (AaaS) where more resource bound
> functions are cast as analytical calculations.
> 
> \item[Security and Privacy.]  We need to deal more stringently with
> security and privacy as part of our scheduling needs which contrasts
> traditional HPC scheduling efforts.
> 
> \item[Fault tolerance and Risk Analysis.] As part of the policy driven
> service level agreements with the cloud providers scheduling must
> include the ability to integrate fault tolerance while leveraging risk
> analysis.
> 
> \item[Traditional Scheduling.] Naturally we need to deal in scheduling
> with traditional issues such as load balancing, congestion, and
> service spikes. However they are amplified by the large resource
> management issues in hyper-scale data centers.
> 
> \item[Edge Computing and Fog computing.] Due to the increased edge and
> Fog computing computational powers available. significant activities
> can also be conducted on edge devices. Billions of cellphones today
> already conduct a significant amount of computation thus scheduling
> must balance between activities that can take place on the edge (Fog)
> or needs to be conducted in the cloud.
787d1360
< \subsection{Future Directions}
789c1362,1370
< In this section, we summarize some of the future directions we need to address.
---
> In this paper, we have surveyed the many important aspects of
> scheduling problems in cloud computing. After introducing the needed
> terminology, we presented a comprehensive taxonomy of the different
> cloud scheduling approaches and issues. A layered and phased
> scheduling model is presented that differentiates the concerns between
> infrastructure, platform, servers and function as a service models. A
> comprehensive investigation has been conducted to verify that the
> taxonomy is valid and that existing scheduling techniques motivate its
> validity.
791d1371
< \begin{description}
793d1372
< \item[Integration for data.] While we focused mostly on compute resource scheduling an additional study is needed to more explicitly address aspects that integrate Big Data and of it while addressing scheduling aspects.
795,797d1373
< \item[Analytics Services.] While FaaS provides the ability to schedule resource-restricted functions the next level of schedulers will address Analytics as a Service (AaaS) where more resource bound functions are cast and exposed to cloud users as analytical calculations.
<  
< \item[Edge Computing.] Due to the increase of computational power of edge devices scheduling algorithms must include the power available on these devices instead of sending all the requests to a cloud. Billions of cellphones today already conduct a significant amount of computation thus scheduling must balance between activities that can take place on the edge or needs to be conducted in the cloud.
799d1374
< \item[Scheduling Challenges Arising form use of Containers.] By using virtualization technologies such as virtual machines the cloud provides the illusion of hardware resources but introduces a cost to also virtualize the operating system.
801d1375
<  Containers, however, use virtualization within the operating system level. Multiple containers run on the top of the operating system kernel. Hence, a container is a lightweight approach to implement the virtualization technology leveraging the underlying OS. The memory consumption by containers is less than the resources required to boot a virtual machine with its virtualized OS. As an example, we point out Kubernetes~\cite{Kuber2018} where containers within a pod~\cite{Kubernates2018} share an IP address and find each other via localhost. Communication among them is done by inter-process communications, such as SystemV semaphores or POSIX shared memory. Containers in different pods cannot communicate directly as they have distinct IP addresses. Kubernetes commonly uses flannel to accomplish container networking. Containers are joined in a virtual network. Kubernetes provides mechanisms to utilize several pre-existing scheduling algorithms but also provides the ability to replace them with customized approaches.
803d1376
<  The challenge here is to assure that containers between users do not create security or violate privacy issues. Also, the access to potentially elevated system privileges may cause other Therefore systems such as Singularity offer users an isolated use of containers within traditional HPC queuing systems to mitigate that issue. Still once on such a system, we still have to be aware of elevated privileges, and containers may only be offered in limited form to its users. Once this has been clarified, also for containers the typical quality assertions during its use apply just as for virtual machines. Such challenges must be integrated into a scheduling strategy when adding containerized cloud resources.
806d1378
< \item[Challenges in Function as a Service.] The {\em Function as a Service} model allows developers to build and execute their requirements. Functions are uploaded to FaaS infrastructure and services and triggered by events. Due to resource limitations, they provide significant information for the underlying layers to provide more efficient resources. However, monitoring tools and fault tolerance have to be carefully integrated to avoid FaaS failures based on resource starvation or an excess of resources used. Also, more intense functions may require splitting them up in smaller so they can be fulfilled resource constraints of the FaaS framework. Naturally, while splitting up a larger model into smaller functions increases the overhead. Such limitations must be understood by the developer in order not to create a function that is impossible to schedule.
808,810d1379
< \end{description}
< 
< \color{black}
812c1381
< In this paper, we have surveyed important classes related to scheduling in cloud computing. After introducing the needed terminology, we presented a comprehensive taxonomy for cloud scheduling including a Y-cloud taxonomy. A layered and phased scheduling model is presented that differentiates the concerns between infrastructure, platform, servers and function as a service model. A comprehensive investigation has been conducted to verify that the taxonomy is valid and that existing scheduling techniques motivate its validity.
---
> \appendix
813a1383
> \section{Postface}
814a1385,1393
> We realize that although we have analyzed a large number of papers,
> there are more papers in that area available. Please inform us as we<
> intend to collect them for further updates to this paper. We like to
> especially pay attention to papers that may motivate us to refine our
> taxonomy. Please send us your reference in \BibTeX format while adding
> in the abstract what your paper provides and how it fits in our
> scheduling taxonomy. The contribution can be send either to
> \verb|laszewski@gmail.com|, or via a github pull request at
> \url{https://github.com/cloudmesh/bib/blob/master/cloud-scheduling.bib}.
816d1394
< \section*{Postface}
818d1395
< We realize that although we have analyzed a large number of papers, there are more papers in that area of cloud scheduling available. The papers cited here are used as examples to showcase some important features related to cloud scheduling. We appreciate it if you inform us about other efforts as we intend to collect them for further updates to this paper. We like to especially pay attention to papers that may motivate us to refine the taxonomy. Please send us your reference in \textsc{Bib}\TeX format while pointing out how your paper enhances the taxonomy. The contribution can be sent either to \verb|laszewski@gmail.com|, or via a GitHub pull request at \url{https://github.com/cyberaide/paper-cloud-scheduling/blob/master/vonlaszewski.bib} and \url{https://github.com/cyberaide/paper-cloud-scheduling/blob/master/cloud-scheduling.bib}
820,822c1397,1399
< \section*{Acknowledgment}
< 
< We like to thank the reviewers for helping us improve this paper with their thoughtful comments.
---
> %
> % BIB
> %
824,825c1401
< %\bibliographystyle{elsarticle-num}
< \bibliographystyle{elsarticle}
---
> \bibliographystyle{elsarticle-num}
830,852d1405
< % \section{Tables}
< 
< \input{table-dynamic-placement}
< \input{table-energy}
< \input{table-network}
< \input{table-cost}
< \input{table-time}
< \input{table-reliability}
< \input{table-security}
< \input{table-heuristic}
< \input{table-AI}
< \input{table-batch}
< \input{table-iaas}
< \input{table-container}
< \input{table-distr-cloud}
< 
< \clearpage
< 
< %\appendix
< 
< %\input{0REVIEW.tex}
< 
< 
854,855d1406
< 
< 
